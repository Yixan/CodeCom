from keras.models import Sequential
from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional,GRU,SimpleRNN
import logging
from keras.callbacks import EarlyStopping
def train(x_train, y_train, x_test, y_test,maxlen,max_token,embedding_matrix,embedding_dims,batch_size,epochs,logpath,modelpath,modelname):
    embedding_layer = Embedding(max_token + 1,embedding_dims,input_length=maxlen,weights=[embedding_matrix],trainable=False)
    print(modelname + 'Build model...')
    model = Sequential()
    model.add(embedding_layer)
    model.add(SimpleRNN(128, activation="relu"))
    model.add(Dropout(0.2))
    model.add(Dense(1, activation='sigmoid'))
    model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])
    early_stopping = EarlyStopping(monitor='val_loss', patience=2)
    print('Train...')
    hist = model.fit(x_train, y_train,batch_size=batch_size,epochs=epochs,lidation_data=(x_test, y_test), callbacks=[early_stopping])
    log_format = "%(asctime)s - %(message)s"
    logging.basicConfig(filename=logpath, level=logging.DEBUG, format=log_format)
    logging.warning(modelname)
    for i in range(len(hist.history["acc"])):
        strlog=str(i+1)+" Epoch "+"-loss: "+str(hist.history["loss"][i])+" -acc: "+str(hist.history["acc"][i])+" -val_loss: "+str(hist.history["val_loss"][i])+" -val_acc: "+str(hist.history["val_acc"][i])
        logging.warning(strlog)
    model.save(modelpath + modelname + '.h5')
def train2(x_train, y_train, x_test, y_test,maxlen,max_token,embedding_matrix,embedding_dims,batch_size,epochs,logpath,modelpath,modelname):
    embedding_layer = Embedding(max_token + 1,embedding_dims,input_length=maxlen,weights=[embedding_matrix],trainable=False)
    print(modelname + 'Build model...')
    model = Sequential()
    model.add(embedding_layer)
    model.add(Dropout(0.2))
    model.add(Dense(1, activation='sigmoid'))
    model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])
    early_stopping = EarlyStopping(monitor='val_loss', patience=2)
    print('Train...')
    hist = model.fit(x_train, y_train,batch_size=batch_size,epochs=epochs,lidation_data=(x_test, y_test), callbacks=[early_stopping])
    log_format = "%(asctime)s - %(message)s"
    logging.basicConfig(filename=logpath, level=logging.DEBUG, format=log_format)
    logging.warning(modelname)
    for i in range(len(hist.history["acc"])):
        strlog=str(i+1)+" Epoch "+"-loss: "+str(hist.history["loss"][i])+" -acc: "+str(hist.history["acc"][i])+" -val_loss: "+str(hist.history["val_loss"][i])+" -val_acc: "+str(hist.history["val_acc"][i])
        logging.warning(strlog)
    model.save(modelpath + modelname + '.h5')
def train3(x_train, y_train, x_test, y_test,maxlen,max_token,embedding_matrix,embedding_dims,batch_size,epochs,logpath,modelpath,modelname):
    embedding_layer = Embedding(max_token + 1,embedding_dims,input_length=maxlen,weights=[embedding_matrix],trainable=False)
    print(modelname+'Build model...')
    model = Sequential()
    model.add(embedding_layer)
    model.add(Bidirectional(GRU(128)))
    model.add(Dropout(0.2))
    model.add(Dense(1, activation='sigmoid'))
    model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])
    early_stopping = EarlyStopping(monitor='val_loss', patience=2)
    print('Train...')
    hist = model.fit(x_train, y_train,batch_size=batch_size,epochs=epochs,lidation_data=(x_test, y_test), callbacks=[early_stopping])
    log_format = "%(asctime)s - %(message)s"
    logging.basicConfig(filename=logpath, level=logging.DEBUG, format=log_format)
    logging.warning(modelname)
    for i in range(len(hist.history["acc"])):
        strlog=str(i+1)+" Epoch "+"-loss: "+str(hist.history["loss"][i])+" -acc: "+str(hist.history["acc"][i])+" -val_loss: "+str(hist.history["val_loss"][i])+" -val_acc: "+str(hist.history["val_acc"][i])
        logging.warning(strlog)
    model.save(modelpath + modelname + '.h5')