import pandas as pd 
import numpy as np
import datetime
import os
from keras.models import Sequential
from keras.layers.core import Dense, Dropout, Activation, Flatten
from keras.layers.recurrent import LSTM, GRU
from keras.layers import Convolution1D, MaxPooling1D
from keras.callbacks import Callback
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
def prepare_data(df,col):
	df__ = df[col]
	dataset = df__.values
	dataset = dataset.astype('float32')
	return dataset
def create_dataset(dataset, look_back=1):
	for i in range(len(dataset)-look_back-1):
		a = dataset[i:(i+look_back), 0]
		dataX.append(a)
		dataY.append(dataset[i + look_back, 0])
	return np.array(dataX), np.array(dataY)
def reshape_dataset(data_,time_series=False):
	if time_series=False:
	else:
	return data_reshape
class LSTM_model
	def __init__(self,trainX,trainY,look_back,batch_size=False, epoch=False,):
		self.trainX = trainX 
		self.trainY = trainY
		self.look_back = look_back
	def simple_LSTM():
		model = Sequential()
		model.add(LSTM(4, input_shape=(1, self.look_back)))
		model.add(Dense(1))
		model.compile(loss='mean_squared_error', optimizer='adam')
		model.fit(self.trainX, self.trainY, epochs=20, batch_size=1, verbose=2)
		return model
	def LSTM_model_memory_batch(batch_size,epoch):
		model = Sequential()
		model.add(LSTM(4, batch_input_shape=(batch_size, self.look_back, 1), stateful=True))
		model.add(Dense(1))
		model.compile(loss='mean_squared_error', optimizer='adam')
		for i in range(epoch):
			model.fit(self.trainX, self.trainY, epochs=1, batch_size=batch_size, verbose=2, shuffle=False)
			model.reset_states()
		return model 
	def Stacked_LSTM_model_memory_batch(batch_size,epoch):
		model = Sequential()
		model.add(LSTM(4, batch_input_shape=(batch_size, self.look_back, 1), stateful=True, return_sequences=True))
		model.add(LSTM(4, batch_input_shape=(batch_size, self.look_back, 1), stateful=True))
		model.add(Dense(1))
		model.compile(loss='mean_squared_error', optimizer='adam')
		for i in range(epoch):
			model.fit(self.trainX, self.trainY, epochs=1, batch_size=batch_size, verbose=2, shuffle=False)
			model.reset_states()
		return model 
def Simple_LSTM(trainX,trainY):
	model = Sequential()
	model.add(LSTM(4, input_shape=(1, look_back)))
	model.add(Dense(1))
	model.compile(loss='mean_squared_error', optimizer='adam')
	model.fit(trainX, trainY, epochs=20, batch_size=1, verbose=2)
	return model
def LSTM_model_memory_batch(trainX,trainY,batch_size,epoch):
	model = Sequential()
	model.add(LSTM(4, batch_input_shape=(batch_size, look_back, 1), stateful=True))
	model.add(Dense(1))
	model.compile(loss='mean_squared_error', optimizer='adam')
	for i in range(epoch):
		model.fit(trainX, trainY, epochs=1, batch_size=batch_size, verbose=2, shuffle=False)
		model.reset_states()
	return model 
def Stacked_LSTM_model_memory_batch(trainX,trainY,batch_size,epoch):
	model = Sequential()
	model.add(LSTM(4, batch_input_shape=(batch_size, look_back, 1), stateful=True, return_sequences=True))
	model.add(LSTM(4, batch_input_shape=(batch_size, look_back, 1), stateful=True))
	model.add(Dense(1))
	model.compile(loss='mean_squared_error', optimizer='adam')
	for i in range(epoch):
		model.fit(trainX, trainY, epochs=1, batch_size=batch_size, verbose=2, shuffle=False)
		model.reset_states()
	return model 
