from tensorflow.keras import backend as K
from tensorflow.keras.models import Sequential
from keras.layers import Dense,Embedding,Flatten,LSTM,Bidirectional,GlobalAveragePooling1D
from keras.layers import Dropout
from keras import regularizers
from keras.layers import Input
from keras.models import Model
from Attention_keras import Attention,Position_Embedding
class WRNNModel:
    def Average(self,inputs_list,weight_array):
        for i in range(1, len(inputs_list)):
            output += inputs_list[i] * weight_array[i]
        return output
    def buildnet(self,trainX, output_d, num_words, embedding_matrix):
        sequence = Input(shape=(trainX.shape[1],), dtype='int32')
        embedded = Embedding(128, 80, input_length=trainX.shape[1], mask_zero=True)(sequence)
        blstm = Bidirectional(LSTM(32, return_sequences=True), merge_mode='sum')(embedded)
        blstm = Bidirectional(LSTM(32))(blstm)
        output = Dense(units = output_d, input_dim=32, use_bias=False,ctivation='sigmoid', name='Model_loss')(blstm)
        print("output",output)
        Main_model = Model(inputs=sequence, outputs=output)   
        return Main_model
class DNNModel:
    def Average(self,inputs_list,weight_array):
        for i in range(1, len(inputs_list)):
            output += inputs_list[i] * weight_array[i]
        return output
    def buildnet(self,trainX, output_d, num_words, embedding_matrix):
        Main_model = Sequential()        
        print ("1layer Embedding shape",Main_model.output_shape)
        Main_model.add(Flatten())
        lstm_hid_size = 128
        l2_rate = 0.05
        Main_model.add(Dense(units=lstm_hid_size, input_dim=lstm_hid_size,kernel_initializer='glorot_uniform',activation='relu',use_bias=False,kernel_regularizer=regularizers.l2(l2_rate)))
        Main_model.add(Dense(units=lstm_hid_size, input_dim=lstm_hid_size,kernel_initializer='glorot_uniform',activation='relu',use_bias=False,kernel_regularizer=regularizers.l2(l2_rate)))
        Main_model.add(Dense(units=lstm_hid_size, input_dim=lstm_hid_size,kernel_initializer='glorot_uniform',activation='relu',use_bias=False)) 
        Main_model.add(Dropout(0.2))
