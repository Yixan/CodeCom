from keras import layers, models, optimizers, initializers
from keras import backend as K
import tensorflow as tf
import numpy as np
from keras.regularizers import l2
def preprocess():
    inp_shape = (100, 127 * 3)
    out_shape = (100, 127)
    inp = layers.Input(shape=inp_shape, name='input')
    def generateCovLayer():
        convModel = models.Sequential(name='conv_layer')
        convModel.add(layers.Conv2D(filters=64, kernel_size=(5, 5), strides=(1, 2), padding='same', input_shape=(100, 127, 3),name='conv1'))
        convModel.add(layers.BatchNormalization())
        convModel.add(layers.Activation('relu'))
        convModel.add(layers.pooling.MaxPooling2D(pool_size=(1, 2), padding='valid'))
        return convModel
    convModel = generateCovLayer()
    def presplit(input):
        return inputR
    return [inp_shape, out_shape, inp, convoutput]
def GenerateBLSTMTime():
    [inp_shape, out_shape, inp, convoutput] = preprocess()
    def easyreshape(x):
        xR = K.reshape(x, shape=[-1, 100, np.prod(convoutput._keras_shape[2::])])
        return xR
    convoutputR = layers.Lambda(easyreshape, name='reshape2')(convoutput)
    SIZE_RLAYERS = 256
    x = convoutputR
        x = layers.Bidirectional(layers.LSTM(SIZE_RLAYERS, return_sequences=True,kernel_regularizer=l2(L2R),recurrent_regularizer=l2(L2R),bias_regularizer=l2(L2R),dropout=DROPOUT,recurrent_dropout=RDROPOUT))(x)
    mask_o = layers.TimeDistributed(layers.Dense(out_shape[-1],activation='sigmoid',kernel_regularizer=l2(L2R),bias_regularizer=l2(L2R)),name='mask_o')(x)
    train_model = models.Model(inputs=[inp], outputs=[mask_o])
    return train_model
def GenerateBLSTMFrequency():
    [inp_shape, out_shape, inp, convoutput] = preprocess()
    SIZE_RLAYERS = 128
    rnnModel = models.Sequential(name='BiLSTM_f')
