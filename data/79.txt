from __future__ import division, print_function
from keras.layers import Dense, Merge, Dropout, RepeatVector
from keras.layers.embeddings import Embedding
from keras.layers.recurrent import LSTM
from keras.layers.recurrent import SimpleRNN
from keras.layers.recurrent import GRU
from keras.models import Sequential
import os
import helper
from argparse import ArgumentParser
parser = ArgumentParser()
parser.add_argument("-t", "--task")
args = parser.parse_args()
BABI_DIR = "data/babi_data/tasks_1-20_v1-2/en"
TASK_NBR = 5
EMBED_HIDDEN_SIZE = 100
BATCH_SIZE = 32
NBR_EPOCHS = 50
train_file, test_file = helper.get_files_for_task(TASK_NBR, BABI_DIR)
data_train = helper.get_stories(os.path.join(BABI_DIR, train_file))
data_test = helper.get_stories(os.path.join(BABI_DIR, test_file))
word2idx = helper.build_vocab([data_train, data_test])
vocab_size = len(word2idx) + 1
print("vocab_size=", vocab_size)
story_maxlen, question_maxlen = helper.get_maxlens([data_train, data_test])
print("story_maxlen=", story_maxlen)
print("question_maxlen=", question_maxlen)
Xs_train, Xq_train, Y_train = helper.vectorize_dualLSTM(data_train, word2idx, tory_maxlen, question_maxlen)
Xs_test, Xq_test, Y_test = helper.vectorize_dualLSTM(data_test, word2idx,tory_maxlen, question_maxlen)
print(Xs_train.shape, Xq_train.shape, Y_train.shape)
print(Xs_test.shape, Xq_test.shape, Y_test.shape)
story_lstm_word = Sequential()
story_lstm_word.add(Embedding(vocab_size, EMBED_HIDDEN_SIZE,input_length=story_maxlen))
story_lstm_word.add(Dropout(0.3))
story_lstm_sentence = Sequential()
story_lstm_sentence.add(Embedding(vocab_size, EMBED_HIDDEN_SIZE,input_length=story_maxlen))
story_lstm_sentence.add(Dropout(0.3))
question_lstm = Sequential()
question_lstm.add(Embedding(vocab_size, EMBED_HIDDEN_SIZE,input_length=question_maxlen))
question_lstm.add(Dropout(0.3))
question_lstm.add(LSTM(EMBED_HIDDEN_SIZE, return_sequences=False))
question_lstm.add(RepeatVector(story_maxlen))
model = Sequential()
model.add(Merge([story_lstm_word ,story_lstm_sentence,question_lstm], mode="sum"))
model.add(LSTM(EMBED_HIDDEN_SIZE, return_sequences=False))
model.add(Dropout(0.3))
model.add(Dense(vocab_size, activation="softmax"))
model.compile(optimizer="adam", loss="categorical_crossentropy", metrics=["accuracy"])
print("Training...")
model.fit([Xs_train, Xs_train, Xq_train], Y_train, tch_size=BATCH_SIZE, nb_epoch=NBR_EPOCHS, validation_split=0.05)
loss, acc = model.evaluate([Xs_test, Xs_test,Xq_test], Y_test, batch_size=BATCH_SIZE)
print()
print("Test loss/accuracy = {:.4f}, {:.4f}".format(loss, acc))