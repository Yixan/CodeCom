from keras.layers import Dense, Dropout, LSTM, Bidirectional
from keras import optimizers
import numpy as np
from keras.wrappers.scikit_learn import KerasClassifier
from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, \
    ExtraTreesClassifier, AdaBoostClassifier, GradientBoostingClassifier, \
    VotingClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from mlxtend.classifier import StackingCVClassifier
"from evaluation import *"
def create_ffNN(lr=0.005, decay=0.001):
    model.add(Dense(100, activation="relu", input_dim=105))
    model.add(Dropout(0.2))
    model.add(Dense(50, activation="relu"))
    model.add(Dropout(0.2))
    model.add(Dense(9, activation="softmax"))
    adam = optimizers.Adam(lr=lr, decay=decay)
    model.compile(loss="categorical_crossentropy",optimizer=adam,metrics=["accuracy"])
    return model
def create_LSTM(optimizer="adam"):
    model.add(LSTM(64, input_shape=(1, 105)))
    model.add(Dropout(0.1))
    model.add(Dense(9, activation="softmax"))
    model.compile(loss="categorical_crossentropy",optimizer=optimizer,metrics=["accuracy"])
    return model
def create_biLSTM(optimizer="adam"):
    model.add(Bidirectional(LSTM(64, input_shape=(1, 105))))
    model.add(Dropout(0.1))
    model.add(Dense(9, activation="softmax"))
    model.compile(loss="categorical_crossentropy",optimizer=optimizer,metrics=["accuracy"])
    return model
def feedforward_models():
        "ffNN": KerasClassifier(build_fn=create_ffNN,tch_size=32, verbose=1, validation_split=0.2)
    }
    params = {
        "ffNN": {"epochs": [35, 40, 45, 80],
                 "lr": [0.005, 0.01],
                 "decay": [0, 0.001]}
    }
    return (models, params)
def recurrent_models():
        "LSTM": KerasClassifier(build_fn=create_LSTM, batch_size=32,erbose=1, validation_split=0.2),
        "biLSTM": KerasClassifier(build_fn=create_biLSTM, batch_size=32,erbose=1, validation_split=0.2)
    }
    params = {
        "LSTM": {"epochs": [120, 150, 180],
                 "optimizer": ["RMSProp", "adam"]},
        "biLSTM": {"epochs": [100, 150, 180],
                   "optimizer": ["RMSProp", "adam"]}
    }
    return (models, params)
def ensemble_models():
        "Bagging": BaggingClassifier(random_state=23),
        "RandomForest": RandomForestClassifier(random_state=23)
    }
    params = {
        "Bagging": {"n_estimators": [15, 30], "max_features": [
            1., 0.9, 0.8], "n_jobs": [1]},
        "RandomForest": {"n_estimators": [10, 20],
                         "criterion": ["gini", "entropy"],
                         "max_features": ["sqrt", "log2"],
                         "n_jobs": [-1]}
    }
    return (models, params)
def best_ensemble():
             "GradBoost", "Voting"]
    bagging_clf = BaggingClassifier(n_estimators=30, max_features=0.8,random_state=23)
    rf_clf = RandomForestClassifier(n_estimators=20, max_features="log2",riterion="entropy", random_state=23)
    return zip(names, [bagging_clf, rf_clf])
def stacking():
    for (_, model) in best_ensemble():
        classifiers.append(model)
    superlearner = RandomForestClassifier(random_state=23)
    np.random.seed(23)
    stacking_clf = StackingCVClassifier(classifiers,eta_classifier=superlearner, cv=5)
    return stacking_clf