import numpy as np
import os
import matplotlib.pyplot as plt
import tqdm
import tensorflow as tf
from six.moves import cPickle as pickle
import random
import math


# Load pickle:
pickle_dir='.'


with open(os.path.join(pickle_dir,"notMNIST.pickle"), 'rb') as f:
	letter_set = pickle.load(f)


train_dataset=letter_set['train_dataset']
train_labels=letter_set['train_labels']
valid_dataset=letter_set['valid_dataset']
valid_labels=letter_set['valid_labels']
test_dataset=letter_set['test_dataset']
test_labels=letter_set['test_labels']


valid_dataset_sani=letter_set['valid_dataset_sani']
valid_labels_sani=letter_set['valid_labels_sani']
test_dataset_sani=letter_set['test_dataset_sani']
test_labels_sani=letter_set['test_labels_sani']




# Reshaping datasets for tensorflow:
# Reformat into a shape that's more adapted to the models we're going to train:
#   -data as a flat matrix,
#	-labels as float 1-hot encodings.


(samples, width, height) = train_dataset.shape
num_labels = 10 # A-J


def reformat(dataset, labels):
	dataset = dataset.reshape((-1, width * height)).astype(np.float32)
	# Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]
	# maps integers to einheitsvektoren
	labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)
	return dataset, labels


train_dataset, train_labels = reformat(train_dataset, train_labels)
valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)
test_dataset, test_labels = reformat(test_dataset, test_labels)


print('Training set', train_dataset.shape, train_labels.shape)
print('Validation set', valid_dataset.shape, valid_labels.shape)
print('Test set', test_dataset.shape, test_labels.shape)


def accuracy(predictions, labels):
	return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))/ predictions.shape[0])




### Problem 1:


# Introduce and tune L2 regularization. L2 amounts to adding a penality on the norm of the weights to the loss.
# Use function nn.l2_loss()


batch_size = 128
n_hidden=1024


# Let's try a few values for beta:
beta_values=[0.0005,0.1,0.00001,0.000001,0.01]
num_steps = 3001


graph = tf.Graph()
with graph.as_default():
	# Input data. For the training data, we use a placeholder that will be fed
	# at run time with a training minibatch.
	tf_train_dataset = tf.placeholder(tf.float32,shape=(batch_size, width * height))
	tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))
	tf_valid_dataset = tf.constant(valid_dataset)
	tf_test_dataset = tf.constant(test_dataset)


	minibatch_dataset = tf.placeholder(tf.float32,shape=(batch_size, width*height))
	minibatch_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))


	l2_regularization_beta = tf.placeholder(tf.float32)


	# Variables.
	weights = tf.Variable(tf.truncated_normal([n_hidden, num_labels],stddev=math.sqrt(2.0/(width*height))))
	biases = tf.Variable(tf.zeros([num_labels]))


	# Hidden Variables.
	weights_hidden = tf.Variable(tf.truncated_normal([width*height,n_hidden],stddev=math.sqrt(2.0/n_hidden)))
	biases_hidden = tf.Variable(tf.zeros([n_hidden]))


	# The hidden layer.
	hidden = tf.nn.relu(tf.matmul(tf_train_dataset,weights_hidden) + biases_hidden)


	# Training computation.
	logits = tf.matmul(hidden, weights) + biases
	loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))


	# With l2 regularization. (only regularize the weights)
	loss=loss+l2_regularization_beta*(tf.nn.l2_loss(weights)+tf.nn.l2_loss(weights_hidden))


	# Optimizer.
	optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)


	# Predictions for the training, validation, and test data.
	train_prediction = tf.nn.softmax(logits)


	hidden_valid = tf.nn.relu(tf.matmul(tf_valid_dataset,weights_hidden)+biases_hidden)
	valid_prediction = tf.nn.softmax(tf.matmul(hidden_valid,weights)+biases)


	hidden_test = tf.nn.relu(tf.matmul(tf_test_dataset,weights_hidden)+biases_hidden)
	test_prediction = tf.nn.softmax(tf.matmul(hidden_test,weights)+biases)


betaplot=[]
accuracyplot=[]


for beta in beta_values:
	with tf.Session(graph=graph) as session:
		tf.global_variables_initializer().run()
		print("Initialized")
		for step in tqdm.trange(num_steps):
			# Randomization across epochs:
			offset=random.randint(0,int((train_labels.shape[0]-batch_size)/batch_size))
			offset=offset*batch_size
			# how to not pick the same batch twice?


			# Generate a minibatch.
			batch_data = train_dataset[offset:(offset + batch_size), :]
			batch_labels = train_labels[offset:(offset + batch_size), :]
			# Prepare a dictionary telling the session where to feed the minibatch.
			# The key of the dictionary is the placeholder node of the graph to be fed,
			# and the value is the numpy array to feed to it.
			feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, l2_regularization_beta:beta}
			_, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)
		betaplot.append(beta)
		accuracyplot.append(accuracy(valid_prediction.eval(), valid_labels))


		print("Beta: {}".format(beta))
		print("Validation accuracy: %.1f%%" % accuracy(valid_prediction.eval(), valid_labels))
		print("Test accuracy: %.1f%%" % accuracy(test_prediction.eval(), test_labels))


# Plot validation accuracy over beta regularization values.
plt.figure(1)
plt.semilogx(betaplot,accuracyplot,'+')
plt.xlabel("Beta")
plt.ylabel("Valid Accuracy")
plt.show()
		


### Problem 2:
# Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?


num_steps = 300
num_batches = 25


# Restrict training data to num_batches batches:
# using just a random list does not prevent overlap
fixed_offset = random.sample(range(int((train_labels.shape[0]-batch_size)/batch_size)),num_batches)
fixed_offset = [i*batch_size for i in fixed_offset]




with tf.Session(graph=graph) as session:
	tf.global_variables_initializer().run()
	print("Initialized")
	#for step in tqdm.trange(num_steps):
	for step in range(num_steps):	
		# Pick one of the prefixed offsets
		offset = random.choice(fixed_offset)


		# Generate a minibatch.
		batch_data = train_dataset[offset:(offset + batch_size), :]
		batch_labels = train_labels[offset:(offset + batch_size), :]


		# Prepare a dictionary telling the session where to feed the minibatch.
		# The key of the dictionary is the placeholder node of the graph to be fed,
		# and the value is the numpy array to feed to it.
		feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels,l2_regularization_beta:0}
		_, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)


		if step==num_steps-1:
			# Calculate Accuarccy of all minibatches: 			
			for idx,offset in enumerate(fixed_offset):
				batch_data = train_dataset[offset:(offset + batch_size), :]
				batch_labels = train_labels[offset:(offset + batch_size), :]


				# Calculate Predictions
				feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}
				predictions = session.run([train_prediction], feed_dict=feed_dict)


				# Display Accuracy for each minibatch:
				print("Minibatch %i accuracy: %.1f%%" % (idx,accuracy(predictions[0], batch_labels)))				
	
			print("Validation accuracy: %.1f%%" % accuracy(valid_prediction.eval(), valid_labels))
			print("Test accuracy: %.1f%%" % accuracy(test_prediction.eval(), test_labels))
	
# As expected the accuracy of the minibatches is close to 100% whereas at the same time the 
# accuracy of the validation and test dataset is much lower. The model is overfitted and memorizes
# the training dataset and does not generalize well to data that it has not seen before.
	


### Problem 3:


# Introduction of dropouts on the hidden layer of the neural network. Use nn.dropout(), make sure to only insert it
# during training.


# Introducing dropout barely improved results! Here, only a two-layer network was used, might have bigger
# impact on deeper networks. 


### Problem 4:


# Use multi-layer model and maximise performance. Add multiple layers and/or introduce learning rate decay.
# The best reported accuracy is 97.1%




##########################################################################
## Stochastic Gradient descent training with L2 regularization and dropout


# Parameters to tune:
batch_size = 128*3
n_hidden=1024*2
dropout_keep_probablity = 1 # if 1 then no dropout
l2_regularization_beta=0.0005
num_steps = 9001


#Learning rate deay parameters:
initial_learning_rate = 0.5
decay_steps = 3500
decay_size = 0.6


graph = tf.Graph()
with graph.as_default():
	# Input data. For the training data, we use a placeholder that will be fed
	# at run time with a training minibatch.
	tf_train_dataset = tf.placeholder(tf.float32,shape=(batch_size, width * height))
	tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))
	tf_valid_dataset = tf.constant(valid_dataset)
	tf_test_dataset = tf.constant(test_dataset)


	minibatch_dataset = tf.placeholder(tf.float32,shape=(batch_size, width*height))
	minibatch_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))


	global_step = tf.Variable(0)  # count the number of steps taken.
	learning_rate = tf.train.exponential_decay(initial_learning_rate, global_step, decay_steps, decay_size,staircase=True)


	# Variables.
	weights = tf.Variable(tf.truncated_normal([n_hidden, num_labels],stddev=math.sqrt(2.0/(width*height))))
	biases = tf.Variable(tf.zeros([num_labels]))


	# Hidden Variables.
	weights_hidden = tf.Variable(tf.truncated_normal([width*height,n_hidden],stddev=math.sqrt(2.0/n_hidden)))
	biases_hidden = tf.Variable(tf.zeros([n_hidden]))


	# The hidden layer.
	#hidden = tf.nn.relu(tf.matmul(tf_train_dataset,weights_hidden) + biases_hidden)


	# With dropout:
	hidden = tf.nn.dropout(tf.nn.relu(tf.matmul(tf_train_dataset,weights_hidden)+biases_hidden),dropout_keep_probablity)


	# Training computation.
	logits = tf.matmul(hidden, weights) + biases
	loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))


	# With l2 regularization. (only regularize the weights)
	loss=loss+l2_regularization_beta*(tf.nn.l2_loss(weights)+tf.nn.l2_loss(weights_hidden))


	# Optimizer.
	#optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)


	# Optimizer with learning rate decay.
	optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)


	# Predictions for the training, validation, and test data.
	train_prediction = tf.nn.softmax(logits)


	hidden_valid = tf.nn.relu(tf.matmul(tf_valid_dataset,weights_hidden)+biases_hidden)
	valid_prediction = tf.nn.softmax(tf.matmul(hidden_valid,weights)+biases)


	hidden_test = tf.nn.relu(tf.matmul(tf_test_dataset,weights_hidden)+biases_hidden)
	test_prediction = tf.nn.softmax(tf.matmul(hidden_test,weights)+biases)




losses=[]
plotsteps=[]
acc=[]
learning_rate_steps=[]




with tf.Session(graph=graph) as session:
	tf.global_variables_initializer().run()
	print("Initialized")
	for step in tqdm.trange(num_steps):
		# Randomization across epochs:
		offset=random.randint(0,int((train_labels.shape[0]-batch_size)/batch_size))
		offset=offset*batch_size


		# Generate a minibatch.
		batch_data = train_dataset[offset:(offset + batch_size), :]
		batch_labels = train_labels[offset:(offset + batch_size), :]


		# Prepare a dictionary telling the session where to feed the minibatch.
		# The key of the dictionary is the placeholder node of the graph to be fed,
		# and the value is the numpy array to feed to it.
		feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}
		_, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)
		if (step % 100 == 0):
			acc.append(accuracy(valid_prediction.eval(), valid_labels))
			losses.append(l)
			plotsteps.append(step)
			learning_rate_steps.append(session.run(learning_rate))


	print("Minibatch accuracy: %.1f%%" % accuracy(predictions, batch_labels))
	print("Validation accuracy: %.1f%%" % accuracy(valid_prediction.eval(), valid_labels))
	print("Test accuracy: %.1f%%" % accuracy(test_prediction.eval(), test_labels))
	


plotoffset=10
f, axarr = plt.subplots(3, sharex=True)
axarr[0].plot(plotsteps[plotoffset:],acc[plotoffset:])
axarr[0].set_title('Accuracy [%]')
axarr[1].plot(plotsteps[plotoffset:],losses[plotoffset:])
axarr[1].set_title("Loss")
axarr[2].plot(plotsteps[plotoffset:],learning_rate_steps[plotoffset:])
axarr[2].set_title("Learning Rate")


plt.show()


# My Result:
# Valid Accuracy 90.5%
# Test Accuracy 96.1%


# Two-layer-model with learning rate decay:


# Parameters: batch_size=384, n_hidden=2048, dropout_keep_probability=1, l2_regularization_beta=0.0005,
#			 num_steps=9001, iniital_learning_rate=0.5, decay_steps=4000, decay_size=0.7







