import tensorflow as tf
import numpy as np
import Utility as util
import math


class SoftMax(object):

    def __init__(self, d, k):
        self.D = d
        self.K = k
        self.NR_VALIDATION_DATA = 50
        self.NR_ITERATION = 1000
        self.SHOW_ACC = 100
        self.BATCH_SIZE = 5000
        self.TRAIN_STEP = 0.01

    def training(self, features, labels):
        features = self.__preprocessing(features)
        train_features = features[self.NR_VALIDATION_DATA:]
        train_labels = labels[self.NR_VALIDATION_DATA:]
        validation_features = features[0:self.NR_VALIDATION_DATA]
        validation_labels = labels[0:self.NR_VALIDATION_DATA]
        sess = tf.InteractiveSession()
        x = tf.placeholder(tf.float32, shape=[None, self.D])
        y_ = tf.placeholder(tf.int64, shape=[None])
        W = tf.Variable(np.random.randn(self.D, self.K) * math.sqrt(2.0 / self.D * self.K), dtype=tf.float32)
        b = tf.Variable(tf.zeros([self.K]), dtype=tf.float32)
        y = tf.nn.softmax(tf.matmul(x, W) + b)
        cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(y, y_)
        cross_entropy_mean = tf.reduce_mean(cross_entropy)
        train_step = tf.train.AdamOptimizer(self.TRAIN_STEP).minimize(cross_entropy_mean)
        correct_prediction = tf.equal(tf.argmax(y, 1), y_)
        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
        sess.run(tf.initialize_all_variables())
        for i in range(self.NR_ITERATION):
            batch = util.generate_batch(train_features, train_labels, self.BATCH_SIZE)
            train_step.run(feed_dict={x: batch[0], y_: batch[1]})
            if i % self.SHOW_ACC == 0:
                value = accuracy.eval(feed_dict={x: validation_features, y_: validation_labels})
                print('Step - ', i, ' - Acc : ', value)
        W_final = W.eval()
        b_final = b.eval()
        sess.close()
        return {'W': W_final, 'b': b_final}

    def predict(self, test_features, test_labels, nn):
        test_features = self.__preprocessing(test_features)
        x = tf.placeholder(tf.float32, shape=[None, self.D])
        W = tf.placeholder(tf.float32, shape=[self.D, self.K])
        b = tf.placeholder(tf.float32, shape=[self.K])
        y = tf.nn.softmax(tf.matmul(x, W) + b)
        sess = tf.InteractiveSession()
        predicted_labels = sess.run(y, feed_dict={x: test_features, W: nn['W'], b: nn['b']})
        sess.close()
        predicted_labels = np.argmax(predicted_labels, axis=1)
        acc = np.mean(predicted_labels == test_labels)
        print('The final accuracy is : ', acc)
        return predicted_labels

    def __preprocessing(self, X):
        X = X.astype(np.float64)
        X = X.T - np.array(np.mean(X, axis=1, dtype=np.float64))
        X = X / np.std(X.T, axis=1, dtype=np.float64)
        X = X.T
        return X


if __name__ == '__main__':
    learn_data = 'result/SoftMax2/cifar_10'
    D = 3072
    K = 10
    softMax = SoftMax(D, K)
    X, y, X_test, y_test = util.load_CIFAR10('data/')
    if util.file_exist(learn_data):
        nn_parameter = util.unpickle(learn_data)
    else:
        nn_parameter = softMax.training(X, y)
        util.pickle_nn(learn_data, nn_parameter)
    predicted_labels = softMax.predict(X_test, y_test, nn_parameter)
    util.save_predicted_labels('result/SoftMax2/submission.csv', predicted_labels)
