import os
import getpass
import sys
import time
import numpy as np
import tensorflow as tf
from q2_initialization import xavier_weight_init
import data_utils.utils as du
import data_utils.ner as ner
from utils import data_iterator
from model import LanguageModel


def variable_summaries(variable, name):
    with tf.name_scope('summaries'):
        mean = tf.reduce_mean(variable)
        tf.summary.scalar('mean/' + name, mean)
        with tf.name_scope('stddev'):
            stddev = tf.sqrt(tf.reduce_sum(tf.square(variable - mean)))
        tf.summary.scalar('stddev/' + name, stddev)
        tf.summary.scalar('max/' + name, tf.reduce_max(variable))
        tf.summary.scalar('min/' + name, tf.reduce_min(variable))
        tf.summary.histogram(name, variable)


class Config(object):
    embed_size = 50
    batch_size = 64
    label_size = 5
    hidden_size = 100
    max_epochs = 10
    early_stopping = 5
    dropout = 0.9
    lr = 0.001
    l2 = 0.001
    window_size = 3


class NERModel(LanguageModel):

    def load_data(self, debug=False):
        self.wv, word_to_num, num_to_word = ner.load_wv('data/ner/vocab.txt', 'data/ner/wordVectors.txt')
        tagnames = ['O', 'LOC', 'MISC', 'ORG', 'PER']
        self.num_to_tag = dict(enumerate(tagnames))
        tag_to_num = {v: k for k, v in self.num_to_tag.items()}
        docs = du.load_dataset('data/ner/train')
        self.X_train, self.y_train = du.docs_to_windows(docs, word_to_num, tag_to_num, wsize=self.config.window_size)
        if debug:
            self.X_train = self.X_train[:1024]
            self.y_train = self.y_train[:1024]
        docs = du.load_dataset('data/ner/dev')
        self.X_dev, self.y_dev = du.docs_to_windows(docs, word_to_num, tag_to_num, wsize=self.config.window_size)
        if debug:
            self.X_dev = self.X_dev[:1024]
            self.y_dev = self.y_dev[:1024]
        docs = du.load_dataset('data/ner/test.masked')
        self.X_test, self.y_test = du.docs_to_windows(docs, word_to_num, tag_to_num, wsize=self.config.window_size)

    def add_placeholders(self):
        self.input_placeholder = tf.placeholder(tf.int32, shape=[None, self.config.window_size])
        self.labels_placeholder = tf.placeholder(tf.float32, shape=[None, self.config.label_size])
        self.dropout_placeholder = tf.placeholder(tf.float32, name='dropout_keep_prob')

    def create_feed_dict(self, input_batch, dropout, label_batch=None):
        feed_dict = {self.input_placeholder: input_batch, self.dropout_placeholder: dropout}
        if label_batch is not None:
            feed_dict[self.labels_placeholder] = label_batch
        return feed_dict

    def add_embedding(self):
        with tf.variable_scope('embedding_layer') as scope:
            embedding = tf.get_variable('embedding', [len(self.wv), self.config.embed_size], initializer=xavier_weight_init())
            window = tf.nn.embedding_lookup(params=embedding, ids=self.input_placeholder)
            window = tf.reshape(window, shape=[-1, self.config.window_size * self.config.embed_size], name='window')
            variable_summaries(window, window.name)
        return window

    def add_model(self, window):
        with tf.variable_scope('layer') as layer_scope:
            W = tf.get_variable('W_l', shape=[self.config.window_size * self.config.embed_size, self.config.hidden_size], initializer=xavier_weight_init())
            b1 = tf.get_variable('b1', shape=[self.config.hidden_size], initializer=tf.constant_initializer(0.0))
            variable_summaries(W, W.name)
            variable_summaries(b1, b1.name)
            with tf.variable_scope('hidden_layer') as hidden_layer:
                U = tf.get_variable('U_h', shape=[self.config.hidden_size, self.config.label_size], initializer=xavier_weight_init())
                b2 = tf.get_variable('b2', shape=[self.config.label_size], initializer=tf.constant_initializer(0.0))
                variable_summaries(U, U.name)
                variable_summaries(b2, b2.name)
        h_fc1 = tf.nn.relu(tf.matmul(window, W) + b1)
        h_fc1 = tf.nn.dropout(h_fc1, self.dropout_placeholder)
        h_fc2 = tf.matmul(h_fc1, U) + b2
        h_fc2 = tf.nn.dropout(h_fc2, self.dropout_placeholder)
        l2_loss = tf.nn.l2_loss(W) + tf.nn.l2_loss(b1) + tf.nn.l2_loss(U) + tf.nn.l2_loss(b2)
        tf.add_to_collection(name='l2_loss', value=l2_loss)
        output = h_fc2
        return output

    def add_loss_op(self, y):
        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y, labels=self.labels_placeholder))
        loss += self.config.l2 * tf.get_collection('l2_loss')[0]
        return loss

    def add_training_op(self, loss):
        train_op = tf.train.AdamOptimizer(self.config.lr).minimize(loss)
        tf.summary.scalar('loss', loss)
        return train_op

    def __init__(self, config):
        self.config = config
        self.load_data(debug=False)
        self.add_placeholders()
        window = self.add_embedding()
        y = self.add_model(window)
        self.loss = self.add_loss_op(y)
        self.predictions = tf.nn.softmax(y)
        one_hot_prediction = tf.argmax(self.predictions, 1)
        correct_prediction = tf.equal(tf.argmax(self.labels_placeholder, 1), one_hot_prediction)
        self.correct_predictions = tf.reduce_sum(tf.cast(correct_prediction, 'int32'))
        self.train_op = self.add_training_op(self.loss)
        self.merged_summaries = tf.summary.merge_all()
        self.summary_writer = None

    def run_epoch(self, session, input_data, input_labels, shuffle=True, verbose=True, epoch=0):
        orig_X, orig_y = input_data, input_labels
        dp = self.config.dropout
        total_loss = []
        total_correct_examples = 0
        total_processed_examples = 0
        total_steps = len(orig_X) / self.config.batch_size
        for step, (x, y) in enumerate(data_iterator(orig_X, orig_y, batch_size=self.config.batch_size, label_size=self.config.label_size, shuffle=shuffle)):
            feed = self.create_feed_dict(input_batch=x, dropout=dp, label_batch=y)
            loss, total_correct, _, merged = session.run([self.loss, self.correct_predictions, self.train_op, self.merged_summaries], feed_dict=feed)
            if step % 50 == 0:
                self.summary_writer.add_summary(merged, epoch * total_steps + step)
            total_processed_examples += len(x)
            total_correct_examples += total_correct
            total_loss.append(loss)
            if verbose and step % verbose == 0:
                sys.stdout.write('\r{} / {} : loss = {}'.format(step, total_steps, np.mean(total_loss)))
                sys.stdout.flush()
        if verbose:
            sys.stdout.write('\r')
            sys.stdout.flush()
        return np.mean(total_loss), total_correct_examples / float(total_processed_examples)

    def predict(self, session, X, y=None):
        dp = 1
        losses = []
        results = []
        if np.any(y):
            data = data_iterator(X, y, batch_size=self.config.batch_size, label_size=self.config.label_size, shuffle=False)
        else:
            data = data_iterator(X, batch_size=self.config.batch_size, label_size=self.config.label_size, shuffle=False)
        for step, (x, y) in enumerate(data):
            feed = self.create_feed_dict(input_batch=x, dropout=dp)
            if np.any(y):
                feed[self.labels_placeholder] = y
                loss, preds = session.run([self.loss, self.predictions], feed_dict=feed)
                losses.append(loss)
            else:
                preds = session.run(self.predictions, feed_dict=feed)
            predicted_indices = preds.argmax(axis=1)
            results.extend(predicted_indices)
        if len(losses) == 0:
            return 0, results
        return np.mean(losses), results


def print_confusion(confusion, num_to_tag):
    total_guessed_tags = confusion.sum(axis=0)
    total_true_tags = confusion.sum(axis=1)
    print()
    print(confusion)
    for i, tag in sorted(num_to_tag.items()):
        prec = confusion[i, i] / float(total_guessed_tags[i])
        recall = confusion[i, i] / float(total_true_tags[i])
        print('Tag: {} - P {:2.4f} / R {:2.4f}'.format(tag, prec, recall))


def calculate_confusion(config, predicted_indices, y_indices):
    confusion = np.zeros((config.label_size, config.label_size), dtype=np.int32)
    for i in range(len(y_indices)):
        correct_label = y_indices[i]
        guessed_label = predicted_indices[i]
        confusion[correct_label, guessed_label] += 1
    return confusion


def save_predictions(predictions, filename):
    with open(filename, 'wb') as f:
        for prediction in predictions:
            f.write(bytes(prediction))


def test_NER():
    config = Config()
    with tf.Graph().as_default():
        model = NERModel(config)
        init = tf.initialize_all_variables()
        saver = tf.train.Saver()
        with tf.Session() as session:
            best_val_loss = float('inf')
            best_val_epoch = 0
            model.summary_writer = tf.summary.FileWriter('NER_train_log/', session.graph)
            session.run(init)
            for epoch in range(config.max_epochs):
                print('Epoch {}'.format(epoch))
                start = time.time()
                train_loss, train_acc = model.run_epoch(session, model.X_train, model.y_train, epoch=epoch)
                val_loss, predictions = model.predict(session, model.X_dev, model.y_dev)
                print('Training loss: {}'.format(train_loss))
                print('Training acc: {}'.format(train_acc))
                print('Validation loss: {}'.format(val_loss))
                if val_loss < best_val_loss:
                    best_val_loss = val_loss
                    best_val_epoch = epoch
                    if not os.path.exists('./weights'):
                        os.makedirs('./weights')
                    saver.save(session, './weights/ner.weights')
                if epoch - best_val_epoch > config.early_stopping:
                    break
                confusion = calculate_confusion(config, predictions, model.y_dev)
                cm = confusion.copy()
                cm = cm.astype(np.float32) / cm.sum(axis=1, keepdims=True)
                cm = cm[(np.newaxis), :, :, (np.newaxis)].astype(np.float32)
                cm_tf_image = tf.convert_to_tensor(cm)
                cm_is = tf.summary.image('confusion_matrix', cm_tf_image)
                cm_current_epoch = session.run(cm_is)
                model.summary_writer.add_summary(cm_current_epoch, epoch)
                print_confusion(confusion, model.num_to_tag)
                print('Total time: {}'.format(time.time() - start))
            saver.restore(session, './weights/ner.weights')
            print('Test')
            print('=-=-=')
            print('Writing predictions to q2_test.predicted')
            _, predictions = model.predict(session, model.X_test, model.y_test)
            save_predictions(predictions, 'q2_test.predicted')


if __name__ == '__main__':
    test_NER()
