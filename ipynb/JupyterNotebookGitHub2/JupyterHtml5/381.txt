
In [ ]:

    
%%time

x_train, x_test, y_train, y_test = train_test_split(X_train, Y_train, test_size = 0.1, random_state = 42)

import tensorflow as tf
from tensorflow.contrib.layers import fully_connected
from sklearn.metrics import roc_auc_score
from functools import partial

training = tf.placeholder_with_default(False, shape=(), name='training')

with tf.Session() as sess:
    
    n_inputs = x_train.shape[1]
    n_hidden1 = 512
    n_hidden2 = 256
    n_hidden3 = 128
    n_outputs = 2
    
    X = tf.placeholder(tf.float32, shape=(None, n_inputs), name="X")
    y = tf.placeholder(tf.int64, shape=(None), name="y")

    my_batch_norm_layer=partial(tf.layers.batch_normalization,training=training, momentum=0.9)
  
    X_drop = tf.layers.dropout(X, 0.2, training=training)
    with tf.name_scope("dnn"):
        hidden1 = tf.layers.dense(X_drop,n_hidden1, kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.1))
        bn1 = my_batch_norm_layer(hidden1)
        bn1_act = tf.nn.elu(bn1)
        hidden1_drop=tf.layers.dropout(bn1_act,rate=0.2,training=training)
        
        hidden2 = tf.layers.dense(hidden1_drop, n_hidden2, kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.1))
        bn2 = my_batch_norm_layer(hidden2)
        bn2_act = tf.nn.elu(bn2)
        hidden2_drop=tf.layers.dropout(bn2_act,rate=0.2,training=training)
        
        hidden3 = tf.layers.dense(hidden2_drop, n_hidden3, kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.1))
        bn3 = my_batch_norm_layer(hidden3)
        bn3_act = tf.nn.elu(bn3)
        hidden3_drop=tf.layers.dropout(bn3_act,rate=0.2,training=training)
        #hidden4 = neuron_layer(hidden3, n_hidden4, name="hidden4",
        #                       activation=tf.nn.relu)
        logits_before_bn = tf.layers.dense(hidden3_drop, n_outputs, activation=tf.nn.relu)
        logits = my_batch_norm_layer(logits_before_bn)

    ratio = 0.08
    class_weight =[ratio, 1.0 - ratio] 
    logits=logits*class_weight
    
    
    with tf.name_scope("loss"):
        xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,logits=logits)
        loss = tf.reduce_mean(xentropy, name="loss")

    learning_rate = 0.01

    with tf.name_scope("train"):
        #optimizer = tf.train.GradientDescentOptimizer(learning_rate)
        optimizer = tf.train.MomentumOptimizer(learning_rate,momentum=0.9)
        training_op = optimizer.minimize(loss)

   
    with  tf.name_scope("eval"):
        correct = tf.nn.in_top_k(logits, y, 1)
        accuracy = tf.reduce_mean(tf.cast(correct, tf.float32)) 
        
    
    init = tf.global_variables_initializer()
    saver = tf.train.Saver()

    n_epochs = 2
    batch_size = 100

    
    with tf.Session() as sess:
        init.run()
        for epoch in range(n_epochs):
            shuffle_indices = np.random.permutation(np.arange(len(y_train)))
            x_train = x_train.iloc[shuffle_indices]
            y_train = y_train[shuffle_indices]
            for iteration in range(len(y_train) // batch_size):
                #X_batch, y_batch = mnist.train.next_batch(batch_size)
                start = iteration * batch_size
                X_batch = x_train[start:start + batch_size]
                y_batch = y_train[start:start + batch_size]
                sess.run(training_op, feed_dict={X: X_batch, y: y_batch})
            acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})
            acc_test = accuracy.eval(feed_dict={X: x_test, y: y_test})
            print(epoch, "Train accuracy:", acc_train, "Test accuracy:", acc_test)
            
            # Check on the AUC
            auc_train = roc_auc_score(y_batch,np.argmax(tf.nn.softmax(logits).eval(feed_dict={X: X_batch}),axis=1))
            auc_test = roc_auc_score(y_test,np.argmax(tf.nn.softmax(logits).eval(feed_dict={X: x_test}),axis=1))
            print(epoch, "Train AUC:", auc_train, "Test AUC:", auc_test)
            print(np.argmax(tf.nn.softmax(logits).eval(feed_dict={X: X_batch}),axis=1))
            print(y_batch)
            #roc_auc_score(y_batch,np.argmax(tf.nn.softmax(logits).eval(feed_dict={X: X_batch})))
        save_path = saver.save(sess, "./my_model_final.ckpt")

    with tf.Session() as sess:
        saver.restore(sess, "./my_model_final.ckpt")
        Z = tf.nn.softmax(logits).eval(feed_dict={X: X_test})


    

