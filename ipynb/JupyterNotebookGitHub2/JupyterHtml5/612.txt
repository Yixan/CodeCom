
In [1]:

    
import tensorflow as tf
import numpy as np

from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets('MNIST_data', one_hot=True)


    



In [2]:

    
def weight_variable(name, shape):
    return tf.get_variable(name, shape, initializer = tf.contrib.layers.xavier_initializer(seed = 1))

def bias_variable(name, shape):
    return tf.get_variable(name, shape, initializer = tf.zeros_initializer())


    



In [3]:

    
tf.reset_default_graph()
x = tf.placeholder(tf.float32, shape = [None, 784])
y_ = tf.placeholder(tf.float32, shape = [None, 10])

w1 = weight_variable('w1', [784,10])
b1 = bias_variable('b1', [10])

z1 = tf.matmul(x, w1) + b1

cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y_, logits = z1))
train_step = tf.train.AdamOptimizer(0.5).minimize(cross_entropy)
correct_prediction = tf.equal(tf.argmax(z1,1), tf.argmax(y_,1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

with tf.Session() as sess:
    init = tf.global_variables_initializer()
    sess.run(init)
    for _ in range(1000):
        batch = mnist.train.next_batch(100)
        if _%100 == 0:
            train_accuracy = accuracy.eval(feed_dict={x:batch[0], y_: batch[1]})
            print("step %d, train accuracy %g"%(_, train_accuracy))
        sess.run(train_step, feed_dict={x: batch[0], y_: batch[1]})
    print("test accuracy %s"% (sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels})))


    



In [4]:

    
tf.reset_default_graph()
x = tf.placeholder(tf.float32, shape = [None, 784])
y_ = tf.placeholder(tf.float32, shape = [None, 10])

w1 = weight_variable('w1', [784,20])
b1 = bias_variable('b1', [20])

z1 = tf.matmul(x, w1) + b1
a1 = tf.nn.relu(z1)

w2 = weight_variable('w2', [20,10])
b2 = bias_variable('b2', [10])

z2 = tf.matmul(a1, w2) + b2

cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y_, logits = z2))
train_step = tf.train.AdamOptimizer(0.001).minimize(cross_entropy)
correct_prediction = tf.equal(tf.argmax(z2,1), tf.argmax(y_,1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

with tf.Session() as sess:
    init = tf.global_variables_initializer()
    sess.run(init)
    for _ in range(1000):
        batch = mnist.train.next_batch(100)
        if _%100 == 0:
            train_accuracy = accuracy.eval(feed_dict={x:batch[0], y_: batch[1]})
            print("step %d, train accuracy %g"%(_, train_accuracy))
        sess.run(train_step, feed_dict={x: batch[0], y_: batch[1]})
    print("test accuracy %s"% (sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels})))


    



In [5]:

    
tf.reset_default_graph()
# Specify that all features have real-value data
feature_columns = [tf.contrib.layers.real_valued_column("", dimension=784)]

classifier = tf.contrib.learn.DNNClassifier(feature_columns=feature_columns,
                                              hidden_units=[20, 10],
                                              n_classes=10,
                                              optimizer=tf.train.AdamOptimizer(learning_rate=0.001),
                                              activation_fn = tf.nn.relu,
                                              model_dir="tmp/mnist_model")

def generate_input_fn(data, label):	
    image_batch, label_batch = tf.train.shuffle_batch(
            [data, label]
            , batch_size=100
            , capacity=800
            , min_after_dequeue=400
            , enqueue_many=True)
    return image_batch, label_batch

def input_fn_for_train():
    train_data = tf.constant(np.array(mnist.train.images, 'float32'))
    train_target = tf.argmax(tf.constant(np.array(mnist.train.labels, 'int64')), axis=1)
    return generate_input_fn(train_data, train_target)

# Fit model.
classifier.fit(input_fn=input_fn_for_train, steps=1000)

# Define the test inputs
def get_test_inputs():
    x = tf.constant(np.array(mnist.test.images, 'float32'))
    y = tf.argmax(tf.constant(np.array(mnist.test.labels, 'int64')), axis=1)

    return x, y

# Evaluate accuracy.
accuracy_score = classifier.evaluate(input_fn=get_test_inputs,
                                       steps=1)["accuracy"]

print("\nTest Accuracy: {0:f}\n".format(accuracy_score))


    



In [6]:

    
tf.reset_default_graph()
x = tf.placeholder(tf.float32, shape = [None, 784])
y_ = tf.placeholder(tf.float32, shape = [None, 10])

w1 = weight_variable('w1', [784,50])
b1 = bias_variable('b1', [50])

z1 = tf.matmul(x, w1) + b1
a1 = tf.nn.relu(z1)

w2 = weight_variable('w2', [50,25])
b2 = bias_variable('b2', [25])

z2 = tf.matmul(a1, w2) + b2
a2 = tf.nn.relu(z2)

w3 = weight_variable('w3', [25,10])
b3 = bias_variable('b3', [10])

z3 = tf.matmul(a2, w3) + b3

cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y_, logits = z3))
train_step = tf.train.AdamOptimizer(0.01).minimize(cross_entropy)
correct_prediction = tf.equal(tf.argmax(z3,1), tf.argmax(y_,1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

with tf.Session() as sess:
    init = tf.global_variables_initializer()
    sess.run(init)
    for _ in range(1000):
        batch = mnist.train.next_batch(100)
        if _%100 == 0:
            train_accuracy = accuracy.eval(feed_dict={x:batch[0], y_: batch[1]})
            print("step %d, train accuracy %g"%(_, train_accuracy))
        sess.run(train_step, feed_dict={x: batch[0], y_: batch[1]})
    print("test accuracy %s"% (sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels})))


    



In [7]:

    
tf.reset_default_graph()
x = tf.placeholder(tf.float32, shape = [None, 784])
y_ = tf.placeholder(tf.float32, shape = [None, 10])
phase = tf.placeholder(tf.bool, name='phase')

w1 = weight_variable('w1', [784,50])
b1 = bias_variable('b1', [50])

z1 = tf.matmul(x, w1) + b1
zbn1 = tf.layers.batch_normalization(z1, training = phase)
a1 = tf.nn.relu(zbn1)

w2 = weight_variable('w2', [50, 25])
b2 = bias_variable('b2', [25])

z2 = tf.matmul(a1, w2) + b2
zbn2 = tf.layers.batch_normalization(z2, training = phase)
a2 = tf.nn.relu(zbn2)

w3 = weight_variable('w3', [25, 10])
b3 = bias_variable('b3', [10])

z3 = tf.matmul(a2, w3) + b3
zbn3 = tf.layers.batch_normalization(z3, training = phase)

cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y_, logits = zbn3))

update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
with tf.control_dependencies(update_ops):
    train_step = tf.train.AdamOptimizer(0.001).minimize(cross_entropy)

correct_prediction = tf.equal(tf.argmax(zbn3,1), tf.argmax(y_,1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

with tf.Session() as sess:
    init = tf.global_variables_initializer()
    sess.run(init)
    for _ in range(2000):
        batch = mnist.train.next_batch(100)
        if _%100 == 0:
            train_accuracy = accuracy.eval(feed_dict={x:batch[0], y_: batch[1], phase:True})
            print("step %d, train accuracy %g"%(_, train_accuracy))
        sess.run([train_step, update_ops], feed_dict={x: batch[0], y_: batch[1], phase: True})
    print("test accuracy %s"% (accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels, phase: False})))


    



In [8]:

    
tf.reset_default_graph()
x = tf.placeholder(tf.float32, shape = [None, 784])
y_ = tf.placeholder(tf.float32, shape = [None, 10])
phase = tf.placeholder(tf.bool, name='phase')
keep_prob = tf.placeholder(tf.float32)

w1 = weight_variable('w1', [784,50])
b1 = bias_variable('b1', [50])

z1 = tf.matmul(x, w1) + b1
zbn1 = tf.layers.batch_normalization(z1, training = phase)
a1 = tf.nn.relu(zbn1)

w2 = weight_variable('w2', [50, 25])
b2 = bias_variable('b2', [25])

z2 = tf.matmul(a1, w2) + b2
zbn2 = tf.layers.batch_normalization(z2, training = phase)
a2 = tf.nn.relu(zbn2)
h_fc2_drop = tf.nn.dropout(a2, keep_prob)


w3 = weight_variable('w3', [25, 10])
b3 = bias_variable('b3', [10])

z3 = tf.matmul(h_fc2_drop, w3) + b3
zbn3 = tf.layers.batch_normalization(z3, training = phase)

cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y_, logits = zbn3))

update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
with tf.control_dependencies(update_ops):
    train_step = tf.train.AdamOptimizer(0.01).minimize(cross_entropy)
    
correct_prediction = tf.equal(tf.argmax(zbn3,1), tf.argmax(y_,1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

with tf.Session() as sess:
    init = tf.global_variables_initializer()
    sess.run(init)
    for _ in range(5000):
        batch = mnist.train.next_batch(100)
        if _%100 == 0:
            train_accuracy = accuracy.eval(feed_dict={x:batch[0], y_: batch[1], phase:True, keep_prob: 0.5})
            print("step %d, train accuracy %g"%(_, train_accuracy))
        sess.run([train_step, update_ops], feed_dict={x: batch[0], y_: batch[1], phase: True, keep_prob: 0.5})
    print("test accuracy %s"% (sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels, phase: False, keep_prob: 1.0})))


    



In [9]:

    
tf.reset_default_graph()
x = tf.placeholder(tf.float32, shape = [None, 784])
y_ = tf.placeholder(tf.float32, shape = [None, 10])

w1 = weight_variable('w1', [784,128])
b1 = bias_variable('b1', [128])

z1 = tf.matmul(x, w1) + b1
a1 = tf.nn.relu(z1)

w2 = weight_variable('w2', [128, 64])
b2 = bias_variable('b2', [64])

z2 = tf.matmul(a1, w2) + b2
a2 = tf.nn.relu(z2)

w3 = weight_variable('w3', [64, 32])
b3 = bias_variable('b3', [32])

z3 = tf.matmul(a2, w3) + b3
a3 = tf.nn.relu(z3)

w4 = weight_variable('w4', [32, 10])
b4 = bias_variable('b4', [10])

z4 = tf.matmul(a3, w4) + b4

cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y_, logits = z4))
train_step = tf.train.AdamOptimizer(0.01).minimize(cross_entropy)
correct_prediction = tf.equal(tf.argmax(z4,1), tf.argmax(y_,1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

with tf.Session() as sess:
    init = tf.global_variables_initializer()
    sess.run(init)
    for _ in range(1000):
        batch = mnist.train.next_batch(100)
        if _%100 == 0:
            train_accuracy = accuracy.eval(feed_dict={x:batch[0], y_: batch[1]})
            print("step %d, train accuracy %g"%(_, train_accuracy))
        sess.run(train_step, feed_dict={x: batch[0], y_: batch[1]})
    print("test accuracy %s"% (sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels})))


    



In [10]:

    
tf.reset_default_graph()
x = tf.placeholder(tf.float32, shape = [None, 784])
y_ = tf.placeholder(tf.float32, shape = [None, 10])
keep_prob = tf.placeholder(tf.float32)

w1 = weight_variable('w1', [784,128])
b1 = bias_variable('b1', [128])

z1 = tf.matmul(x, w1) + b1
a1 = tf.nn.relu(z1)

w2 = weight_variable('w2', [128, 64])
b2 = bias_variable('b2', [64])

z2 = tf.matmul(a1, w2) + b2
a2 = tf.nn.relu(z2)

w3 = weight_variable('w3', [64, 32])
b3 = bias_variable('b3', [32])

z3 = tf.matmul(a2, w3) + b3
a3 = tf.nn.relu(z3)
h_fc3_drop = tf.nn.dropout(a3, keep_prob)

w4 = weight_variable('w4', [32, 10])
b4 = bias_variable('b4', [10])

z4 = tf.matmul(h_fc3_drop, w4) + b4


cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y_, logits = z4))
train_step = tf.train.AdamOptimizer(1e-3).minimize(cross_entropy)
correct_prediction = tf.equal(tf.argmax(z4,1), tf.argmax(y_,1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

with tf.Session() as sess:
    init = tf.global_variables_initializer()
    sess.run(init)
    for _ in range(5000):
        batch = mnist.train.next_batch(100)
        if _%100 == 0:
            train_accuracy = accuracy.eval(feed_dict={x:batch[0], y_: batch[1], keep_prob: 0.5})
            print("step %d, train accuracy %g"%(_, train_accuracy))
        sess.run(train_step, feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})
    print("test accuracy %s"% (sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0})))


    



In [11]:

    
tf.reset_default_graph()
x = tf.placeholder(tf.float32, shape = [None, 784])
y_ = tf.placeholder(tf.float32, shape = [None, 10])
keep_prob = tf.placeholder(tf.float32)

w1 = weight_variable('w1', [784,128])
b1 = bias_variable('b1', [128])

z1 = tf.matmul(x, w1) + b1
a1 = tf.nn.relu(z1)
h_fc1_drop = tf.nn.dropout(a1, keep_prob)

w2 = weight_variable('w2', [128, 64])
b2 = bias_variable('b2', [64])

z2 = tf.matmul(h_fc1_drop, w2) + b2
a2 = tf.nn.relu(z2)
h_fc2_drop = tf.nn.dropout(a2, keep_prob)

w3 = weight_variable('w3', [64, 32])
b3 = bias_variable('b3', [32])

z3 = tf.matmul(h_fc2_drop, w3) + b3
a3 = tf.nn.relu(z3)
h_fc3_drop = tf.nn.dropout(a3, keep_prob)

w4 = weight_variable('w4', [32, 10])
b4 = bias_variable('b4', [10])

z4 = tf.matmul(h_fc3_drop, w4) + b4


cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y_, logits = z4))
train_step = tf.train.AdamOptimizer(0.01).minimize(cross_entropy)
correct_prediction = tf.equal(tf.argmax(z4,1), tf.argmax(y_,1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

with tf.Session() as sess:
    init = tf.global_variables_initializer()
    sess.run(init)
    for _ in range(3000):
        batch = mnist.train.next_batch(100)
        if _%100 == 0:
            train_accuracy = accuracy.eval(feed_dict={x:batch[0], y_: batch[1], keep_prob: 0.5})
            print("step %d, train accuracy %g"%(_, train_accuracy))
        sess.run(train_step, feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})
    print("test accuracy %s"% (sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0})))


    



In [12]:

    
tf.reset_default_graph()
x = tf.placeholder(tf.float32, shape = [None, 784])
y_ = tf.placeholder(tf.float32, shape = [None, 10])
keep_prob = tf.placeholder(tf.float32)

W_conv1 = weight_variable('W_conv1', [5, 5, 1, 32])
b_conv1 = bias_variable('b_conv1', [32])

x_image = tf.reshape(x, [-1,28,28,1])

h_conv1 = tf.nn.relu(tf.nn.conv2d(input = x_image, filter = W_conv1, strides = [1,1,1,1], padding = 'SAME') + b_conv1)
h_pool1 = tf.nn.max_pool(value = h_conv1, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'SAME')

W_conv2 = weight_variable('W_conv2', [5, 5, 32, 64])
b_conv2 = bias_variable('b_conv2', [64])

h_conv2 = tf.nn.relu(tf.nn.conv2d(input = h_pool1, filter = W_conv2, strides = [1,1,1,1], padding = 'SAME') + b_conv2)
h_pool2 = tf.nn.max_pool(value = h_conv2, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'SAME')

W_fc1 = weight_variable('W_fc1', [7 * 7 * 64, 1024])
b_fc1 = bias_variable('b_fc1', [1024])

h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])
h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)

h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)

W_fc2 = weight_variable('W_fc2', [1024, 10])
b_fc2 = bias_variable('b_fc2', [10])

y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2

cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))
train_step = tf.train.AdamOptimizer(1e-3).minimize(cross_entropy)
correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

with tf.Session() as sess:
    init = tf.global_variables_initializer()
    sess.run(init)
    for _ in range(300):
        batch = mnist.train.next_batch(100)
        if _%100 == 0:
            train_accuracy = accuracy.eval(feed_dict={x:batch[0], y_: batch[1], keep_prob: 0.5})
            print("step %d, train accuracy %g"%(_, train_accuracy))
        sess.run(train_step, feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})
    print("test accuracy %s"% (sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0})))


    



In [13]:

    
tf.reset_default_graph()
x = tf.placeholder(tf.float32, shape = [None, 784])
y_ = tf.placeholder(tf.float32, shape = [None, 10])
keep_prob = tf.placeholder(tf.float32)

W_conv1 = weight_variable('W_conv1', [3, 3, 1, 32])
b_conv1 = bias_variable('b_conv1', [32])

x_image = tf.reshape(x, [-1,28,28,1])

h_conv1 = tf.nn.relu(tf.nn.conv2d(input = x_image, filter = W_conv1, strides = [1,1,1,1], padding = 'SAME') + b_conv1)
h_pool1 = tf.nn.max_pool(value = h_conv1, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'SAME')

W_conv2 = weight_variable('W_conv2', [5, 5, 32, 64])
b_conv2 = bias_variable('b_conv2', [64])

h_conv2 = tf.nn.relu(tf.nn.conv2d(input = h_pool1, filter = W_conv2, strides = [1,1,1,1], padding = 'SAME') + b_conv2)
h_pool2 = tf.nn.max_pool(value = h_conv2, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'SAME')

W_fc1 = weight_variable('W_fc1', [7 * 7 * 64, 1024])
b_fc1 = bias_variable('b_fc1', [1024])

h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])
h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)

h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)

W_fc2 = weight_variable('W_fc2', [1024, 10])
b_fc2 = bias_variable('b_fc2', [10])

y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2

cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))
train_step = tf.train.AdamOptimizer(1e-3).minimize(cross_entropy)
correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

with tf.Session() as sess:
    init = tf.global_variables_initializer()
    sess.run(init)
    for _ in range(300):
        batch = mnist.train.next_batch(100)
        if _%100 == 0:
            train_accuracy = accuracy.eval(feed_dict={x:batch[0], y_: batch[1], keep_prob: 0.5})
            print("step %d, train accuracy %g"%(_, train_accuracy))
        sess.run(train_step, feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})
    print("test accuracy %s"% (sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0})))


    



In [ ]:

    
tf.reset_default_graph()
x = tf.placeholder(tf.float32, shape = [None, 784])
y_ = tf.placeholder(tf.float32, shape = [None, 10])

W_conv1 = weight_variable('W_conv1', [3, 3, 1, 32])
b_conv1 = bias_variable('b_conv1', [32])

x_image = tf.reshape(x, [-1,28,28,1])

h_conv1 = tf.nn.relu(tf.nn.conv2d(input = x_image, filter = W_conv1, strides = [1,1,1,1], padding = 'SAME') + b_conv1)
h_pool1 = tf.nn.max_pool(value = h_conv1, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'SAME')

W_conv2 = weight_variable('W_conv2', [5, 5, 32, 64])
b_conv2 = bias_variable('b_conv2', [64])

h_conv2 = tf.nn.relu(tf.nn.conv2d(input = h_pool1, filter = W_conv2, strides = [1,1,1,1], padding = 'SAME') + b_conv2)
# h_pool2 = tf.nn.max_pool(value = h_conv2, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'SAME')

W_skconv2 = weight_variable('W_skconv2', [2, 2, 1, 64])
b_skconv2 = bias_variable('b_skconv2', [64])

x_skip1 = tf.nn.conv2d(input = x_image, filter = W_skconv2, strides = [1,2,2,1], padding = 'VALID') + b_skconv2

W_conv3 = weight_variable('W_conv3', [3, 3, 64, 128])
b_conv3 = bias_variable('b_conv3', [128])

h_conv2_add = tf.add(h_conv2, x_skip1)

h_conv3 = tf.nn.relu(tf.nn.conv2d(input = h_conv2_add, filter = W_conv3, strides = [1,1,1,1], padding = 'SAME') + b_conv3)
h_pool3 = tf.nn.max_pool(value = h_conv3, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'SAME')

W_fc1 = weight_variable('W_fc1', [7 * 7 * 128, 1024])
b_fc1 = bias_variable('b_fc1', [1024])

h_pool3_flat = tf.reshape(h_pool3, [-1, 7*7*128])
h_fc1 = tf.nn.relu(tf.matmul(h_pool3_flat, W_fc1) + b_fc1)

keep_prob = tf.placeholder(tf.float32)
h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)

W_fc2 = weight_variable('W_fc2', [1024, 10])
b_fc2 = bias_variable('b_fc2', [10])

y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2

cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))
train_step = tf.train.AdamOptimizer(1e-3).minimize(cross_entropy)
correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

with tf.Session() as sess:
    init = tf.global_variables_initializer()
    sess.run(init)
    for _ in range(500):
        batch = mnist.train.next_batch(100)
        if _%100 == 0:
            train_accuracy = accuracy.eval(feed_dict={x:batch[0], y_: batch[1], keep_prob: 0.5})
            print("step %d, train accuracy %g"%(_, train_accuracy))
        sess.run(train_step, feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})
    print("test accuracy %s"% (sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0})))


    



In [ ]:

    
 


    

