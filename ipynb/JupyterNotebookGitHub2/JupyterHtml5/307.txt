
In [1]:

    
import tensorflow as tf
import pandas as pd
import numpy as np
import math
import random
from tqdm import trange


    



In [2]:

    
data2 = pd.read_csv("white.csv")


    



In [3]:

    
data2 = pd.concat([data2.iloc[:,0:11], pd.get_dummies(data2["y"])], axis = 1)


    



In [4]:

    
def minimini(x, size, seed = 0): # 미니배치 구현
    np.random.seed(seed)
    m = x.shape[0]
    mini = []
    per = list(np.random.permutation(m))
    shuffle_x = x.iloc[per, :]
    num = math.floor(m/size)
    for k in range(num):
        mini.append(shuffle_x.iloc[k*size:(k+1)*size, : ])
    if m % size != 0:
        mini.append(shuffle_x.iloc[num*size:m, :])
    return mini


    



In [49]:

    
#1
dim = 11
learn_rate = 0.001
train_epoch = 101
batch_size = 128

X = tf.placeholder(tf.float32, shape = [None, 11])
Y = tf.placeholder(tf.float32, shape = [None, 7])

w0 = tf.Variable(tf.zeros(shape = [dim, 256]))
w1 = tf.Variable(tf.zeros(shape = [256, 128]))
w2 = tf.Variable(tf.zeros(shape = [128, 7]))

b0 = tf.Variable(tf.zeros(shape = [256]))
b1 = tf.Variable(tf.zeros(shape = [128]))
b2 = tf.Variable(tf.zeros(shape = [7]))

def classifier(x):
    c1 = tf.nn.relu(tf.add(tf.matmul(x, w0), b0))
    c2 = tf.nn.relu(tf.add(tf.matmul(c1, w1), b1))
    return tf.nn.sigmoid(tf.add(tf.matmul(c2, w2), b2))

y = classifier(X)
cost = -tf.reduce_sum(Y*tf.log(y))
optimizer = tf.train.GradientDescentOptimizer(learn_rate).minimize(cost)

sess = tf.Session()
sess.run(tf.global_variables_initializer())
for i in range(train_epoch):
    avg_cost = 0
    a = minimini(data2, batch_size, seed = i)
    gr = list(np.random.permutation(math.floor(data2.shape[0]/batch_size)))
    for step in range(math.floor(data2.shape[0]/batch_size)):
            temp = a[gr[step]]
            xs = temp.iloc[:, 0:11]
            ys = temp.iloc[:, 11:]
            _, c = sess.run([optimizer, cost], feed_dict = {X: xs, Y: ys})
            avg_cost += c/math.floor(data2.shape[0]/batch_size)
    if i % 10  == 0:
        print("epoch"+str(i), avg_cost)
sess.close()


    



In [48]:

    
#2
dim = 11
learn_rate = 0.001
train_epoch = 101
batch_size = 128

X = tf.placeholder(tf.float32, shape = [None, 11])
Y = tf.placeholder(tf.float32, shape = [None, 7])

w0 = tf.Variable(tf.zeros(shape = [dim, 512]))
w1 = tf.Variable(tf.zeros(shape = [512, 256]))
w2 = tf.Variable(tf.zeros(shape = [256, 128]))
w3 = tf.Variable(tf.zeros(shape = [128, 7]))

b0 = tf.Variable(tf.zeros(shape = [512]))
b1 = tf.Variable(tf.zeros(shape = [256]))
b2 = tf.Variable(tf.zeros(shape = [128]))
b3 = tf.Variable(tf.zeros(shape = [7]))

def classifier(x):
    c1 = tf.nn.relu(tf.add(tf.matmul(x, w0), b0))
    c2 = tf.nn.relu(tf.add(tf.matmul(c1, w1), b1))
    c3 = tf.nn.relu(tf.add(tf.matmul(c2, w2), b2))
    return tf.nn.sigmoid(tf.add(tf.matmul(c3, w3), b3))

y = classifier(X)
cost = -tf.reduce_sum(Y*tf.log(y))
optimizer = tf.train.AdamOptimizer(learn_rate).minimize(cost)

sess = tf.Session()
sess.run(tf.global_variables_initializer())
for i in range(train_epoch):
    avg_cost = 0
    a = minimini(data2, batch_size, seed = i)
    gr = list(np.random.permutation(math.floor(data2.shape[0]/batch_size)))
    for step in range(math.floor(data2.shape[0]/batch_size)):
            temp = a[gr[step]]
            xs = temp.iloc[:, 0:11]
            ys = temp.iloc[:, 11:]
            _, c = sess.run([optimizer, cost], feed_dict = {X: xs, Y: ys})
            avg_cost += c/math.floor(data2.shape[0]/batch_size)
    if i % 10  == 0:
        print("epoch"+str(i), avg_cost)
sess.close()


    



In [51]:

    
#3
dim = 11
learn_rate = 0.001
train_epoch = 31
batch_size = 128

X = tf.placeholder(tf.float32, shape = [None, 11])
Y = tf.placeholder(tf.float32, shape = [None, 7])

def xavier(size):
    std = 1. / tf.sqrt(size[0]/2.)
    return tf.random_normal(shape = size, stddev = std)

w0 = tf.Variable(xavier(size = [dim, 512]))
w1 = tf.Variable(xavier(size = [512, 256]))
w2 = tf.Variable(xavier(size = [256, 128]))
w3 = tf.Variable(xavier(size = [128, 7]))

b0 = tf.Variable(tf.zeros(shape = [512]))
b1 = tf.Variable(tf.zeros(shape = [256]))
b2 = tf.Variable(tf.zeros(shape = [128]))
b3 = tf.Variable(tf.zeros(shape = [7]))

def classifier(x):
    c1 = tf.nn.relu(tf.add(tf.matmul(x, w0), b0))
    c2 = tf.nn.relu(tf.add(tf.matmul(c1, w1), b1))
    c3 = tf.nn.relu(tf.add(tf.matmul(c2, w2), b2))
    return tf.nn.relu(tf.add(tf.matmul(c3, w3), b3))

y = classifier(X)
cost = -tf.reduce_sum(Y*tf.log(y + 1e-6))
optimizer = tf.train.AdamOptimizer(learn_rate).minimize(cost)

sess = tf.Session()
sess.run(tf.global_variables_initializer())
for i in range(train_epoch):
    avg_cost = 0
    a = minimini(data2, batch_size, seed = i)
    gr = list(np.random.permutation(math.floor(data2.shape[0]/batch_size)))
    for step in range(math.floor(data2.shape[0]/batch_size)):
            temp = a[gr[step]]
            xs = temp.iloc[:, 0:11]
            ys = temp.iloc[:, 11:]
            _, c = sess.run([optimizer, cost], feed_dict = {X: xs, Y: ys})
            avg_cost += c/math.floor(data2.shape[0]/batch_size)
    if i % 10  == 0:
        print("epoch"+str(i), avg_cost)
sess.close()


    



In [54]:

    
#4
dim = 11
learn_rate = 0.001
train_epoch = 101
batch_size = 128

X = tf.placeholder(tf.float32, shape = [None, 11])
Y = tf.placeholder(tf.float32, shape = [None, 7])
keep_prob = tf.placeholder(tf.float32)

w0 = tf.Variable(tf.zeros(shape = [dim, 512]))
w1 = tf.Variable(tf.zeros(shape = [512, 256]))
w2 = tf.Variable(tf.zeros(shape = [256, 128]))
w3 = tf.Variable(tf.zeros(shape = [128, 7]))

b0 = tf.Variable(tf.zeros(shape = [512]))
b1 = tf.Variable(tf.zeros(shape = [256]))
b2 = tf.Variable(tf.zeros(shape = [128]))
b3 = tf.Variable(tf.zeros(shape = [7]))


def classifier(x, keep_prob):
    c1 = tf.nn.relu(tf.add(tf.matmul(x, w0), b0))
    c2 = tf.nn.relu(tf.add(tf.matmul(c1, w1), b1))
    c3 = tf.nn.relu(tf.add(tf.matmul(c2, w2), b2))
    c_drop = tf.nn.dropout(c3, keep_prob)
    return tf.nn.sigmoid(tf.add(tf.matmul(c_drop, w3), b3))

y = classifier(X, keep_prob)
cost = -tf.reduce_sum(Y*tf.log(y))
optimizer = tf.train.GradientDescentOptimizer(learn_rate).minimize(cost)

sess = tf.Session()
sess.run(tf.global_variables_initializer())
for i in range(train_epoch):
    avg_cost = 0
    a = minimini(data2, batch_size, seed = i)
    gr = list(np.random.permutation(math.floor(data2.shape[0]/batch_size)))
    for step in range(math.floor(data2.shape[0]/batch_size)):
            temp = a[gr[step]]
            xs = temp.iloc[:, 0:11]
            ys = temp.iloc[:, 11:]
            _, c = sess.run([optimizer, cost], feed_dict = {X: xs, Y: ys, keep_prob: 0.7})
            avg_cost += c/math.floor(data2.shape[0]/batch_size)
    if i % 10  == 0:
        print("epoch"+str(i), avg_cost)
sess.close()


    



In [7]:

    
#5
dim = 11
learn_rate = 0.01
train_epoch = 101
batch_size = 128

X = tf.placeholder(tf.float32, shape = [None, 11])
Y = tf.placeholder(tf.float32, shape = [None, 7])
keep_prob = tf.placeholder(tf.float32)

def xavier(size):
    std = 1. / tf.sqrt(size[0]/2.)
    return tf.random_normal(shape = size, stddev = std)

w0 = tf.Variable(tf.zeros(shape = [dim, 256]))
w1 = tf.Variable(tf.zeros(shape = [256, 128]))
w2 = tf.Variable(tf.zeros(shape = [128, 7]))

b0 = tf.Variable(tf.zeros(shape = [256]))
b1 = tf.Variable(tf.zeros(shape = [128]))
b2 = tf.Variable(tf.zeros(shape = [7]))

def classifier(x, keep_prob):
    c1 = tf.nn.elu(tf.add(tf.matmul(x, w0), b0))
    c2 = tf.nn.elu(tf.add(tf.matmul(c1, w1), b1))
    c_drop = tf.nn.dropout(c2, keep_prob)
    return tf.nn.elu(tf.add(tf.matmul(c_drop, w2), b2))

y = classifier(X, keep_prob)

cost = -tf.reduce_sum(Y*tf.log(y + 1e-5))
optimizer = tf.train.AdamOptimizer(learn_rate).minimize(cost)

sess = tf.Session()
sess.run(tf.global_variables_initializer())
for i in range(train_epoch):
    avg_cost = 0
    a = minimini(data2, batch_size, seed = i)
    gr = list(np.random.permutation(math.floor(data2.shape[0]/batch_size)))
    for step in range(math.floor(data2.shape[0]/batch_size)):
            temp = a[gr[step]]
            xs = temp.iloc[:, 0:11]
            ys = temp.iloc[:, 11:]
            _, c = sess.run([optimizer, cost], feed_dict = {X: xs, Y: ys, keep_prob: 0.1})
            avg_cost += c/math.floor(data2.shape[0]/batch_size)
    if i % 10  == 0:
        print("epoch"+str(i), avg_cost)
sess.close()


    



In [ ]:

    
 


    

