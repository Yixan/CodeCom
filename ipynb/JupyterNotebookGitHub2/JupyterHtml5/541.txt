
In [1]:

    
# These are all the modules we'll be using later. Make sure you can import them
# before proceeding further.
from __future__ import print_function
import numpy as np
import tensorflow as tf
from six.moves import cPickle as pickle


    



In [2]:

    
pickle_file = 'notMNIST_sanit.pickle'

with open(pickle_file, 'rb') as f:
  save = pickle.load(f)
  train_dataset = save['train_dataset']
  train_labels = save['train_labels']
  valid_dataset = save['valid_dataset']
  valid_labels = save['valid_labels']
  test_dataset = save['test_dataset']
  test_labels = save['test_labels']
  del save  # hint to help gc free up memory
  print('Training set', train_dataset.shape, train_labels.shape)
  print('Validation set', valid_dataset.shape, valid_labels.shape)
  print('Test set', test_dataset.shape, test_labels.shape)


    



In [3]:

    
image_size = 28
num_labels = 10

def reformat(dataset, labels):
  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)
  # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]
  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)
  return dataset, labels
train_dataset, train_labels = reformat(train_dataset, train_labels)
valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)
test_dataset, test_labels = reformat(test_dataset, test_labels)
print('Training set', train_dataset.shape, train_labels.shape)
print('Validation set', valid_dataset.shape, valid_labels.shape)
print('Test set', test_dataset.shape, test_labels.shape)


    



In [4]:

    
def accuracy(predictions, labels):
  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))
          / predictions.shape[0])


    



In [5]:

    
batch_size = 128
beta1 = 0.005
beta2 = 0.005
hidden_layer_size = 1024*2

graph = tf.Graph()
with graph.as_default():  
  tf_train_dataset = tf.placeholder(tf.float32,
                                  shape=(batch_size, image_size * image_size))
  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))
  tf_test_dataset = tf.constant(test_dataset)
  tf_valid_dataset = tf.constant(valid_dataset)


  # Variables.
  weights = tf.Variable(
  tf.truncated_normal([image_size * image_size, hidden_layer_size]))
  biases = tf.Variable(tf.zeros([hidden_layer_size]))
  # Variables2,
  weights2 = tf.Variable(
  tf.truncated_normal([hidden_layer_size, num_labels]))
  biases2 = tf.Variable(tf.zeros([num_labels]))
  
  # Training computation.
  logits =  tf.matmul(tf_train_dataset, weights) + biases
  logits2 = tf.matmul(tf.nn.relu(logits),weights2) + biases2
  loss = tf.reduce_mean(
    tf.nn.softmax_cross_entropy_with_logits_v2(
      labels=tf_train_labels, logits=logits2)) + beta1*tf.nn.l2_loss(weights) + beta2*tf.nn.l2_loss(weights2)
  # Optimizer.
  optimizer = tf.train.GradientDescentOptimizer(0.15).minimize(loss)
  # Predictions for the training, validation, and test data.
  train_prediction = tf.nn.softmax(logits2)
  valid_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, weights) + biases),weights2)+biases2)
  test_prediction =  tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, weights) + biases),weights2)+biases2)
    


    



In [6]:

    
num_steps = 3001

with tf.Session(graph=graph) as session:
  tf.global_variables_initializer().run()
  print("Initialized")
  for step in range(num_steps):
    offset = batch_size*step%(train_labels.shape[0] - batch_size)
    batch_data   = train_dataset[offset:(offset+batch_size),:]
    batch_labels = train_labels[offset:(offset+batch_size),:]
    # Prepare a dictionary telling the session where to feed the minibatch.
    # The key of the dictionary is the placeholder node of the graph to be fed,
    # and the value is the numpy array to feed to it.
    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}
    _,l,pred = session.run([optimizer,loss,train_prediction],feed_dict=feed_dict)
    if step%500==0:
      print ("loss = ", l)
      print("Batch accuracy: ", accuracy(pred,batch_labels))
      print("validation acc: ", accuracy(valid_prediction.eval(), valid_labels))
  print("test ",accuracy(test_prediction.eval(),test_labels))


    



In [80]:

    
batch_size = 128
beta1 = 0.001
beta2 = 0.001
beta3 = 0.001
beta4 = 0.001/2
beta5 = 0.001/2

hidden_layer_size = 28*28
hidden_layer_size2 = 1024*2
hidden_layer_size3 = 1024*2
hidden_layer_size4 = 28*28

graph = tf.Graph()
with graph.as_default():  
  tf_train_dataset = tf.placeholder(tf.float32,
                                  shape=(batch_size, image_size * image_size))
  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))
  tf_test_dataset = tf.constant(test_dataset)
  tf_valid_dataset = tf.constant(valid_dataset)


  #input vector to hidden 1
  weights = tf.Variable(
  tf.truncated_normal([image_size * image_size, hidden_layer_size]))
  biases = tf.Variable(tf.zeros([hidden_layer_size]))
  # hidden 1 to hidden 2,
  weights2 = tf.Variable(
  tf.truncated_normal([hidden_layer_size, hidden_layer_size2]))
  biases2 = tf.Variable(tf.zeros([hidden_layer_size2]))
  # hidden 3 to hidden4,
  weights3 = tf.Variable(
  tf.truncated_normal([hidden_layer_size2, hidden_layer_size3]))
  biases3 = tf.Variable(tf.zeros([hidden_layer_size3]))
  # hidden 4 to output,
  weights4 = tf.Variable(
  tf.truncated_normal([hidden_layer_size3, hidden_layer_size4]))
  biases4 = tf.Variable(tf.zeros([hidden_layer_size4]))
  # hidden 4 to output,
  weights5 = tf.Variable(
  tf.truncated_normal([hidden_layer_size4, num_labels]))
  biases5 = tf.Variable(tf.zeros([num_labels]))
  
  # Training computation.
  logits =  tf.matmul(tf_train_dataset, weights)  + biases
  logits = tf.matmul(tf.nn.relu(logits),weights2) + biases2
  logits = tf.matmul(tf.nn.relu(logits),weights3) + biases3
  logits = tf.matmul(tf.nn.relu(logits),weights4) + biases4
  logits = tf.matmul(tf.nn.relu(logits),weights5) + biases5
  # Training computation with dropouts

  #logits_ =  tf.nn.dropout(tf.matmul(tf_train_dataset, weights) + biases,tf.constant(0.95))
  #logits2 = tf.nn.dropout(tf.matmul(tf.nn.relu(logits_),weights2) + biases2,tf.constant(0.95))
  #logits3 = tf.nn.dropout(tf.matmul(tf.nn.relu(logits2),weights3) + biases3,tf.constant(0.95))
    
  loss = tf.reduce_mean(
  tf.nn.softmax_cross_entropy_with_logits_v2(
    labels=tf_train_labels, logits=logits)) + beta1*tf.nn.l2_loss(
      weights) + beta2*tf.nn.l2_loss(weights2)+ beta3*tf.nn.l2_loss(weights3) +beta4*tf.nn.l2_loss(weights4)+beta5*tf.nn.l2_loss(weights5)

  # Optimizer.
  global_step = tf.Variable(0)  # count the number of steps taken.
  #learning_rate = tf.train.exponential_decay(0.0000004, global_step,500,0.95,staircase=True)
  optimizer = tf.train.GradientDescentOptimizer(0.00000035).minimize(loss)#, global_step=global_step)
  
  
  # Predictions for the training, validation, and test data.
  train_prediction = tf.nn.softmax(logits)


  valid_prediction = tf.nn.relu(tf.matmul(tf_valid_dataset, weights) + biases)
  valid_prediction = tf.nn.relu(tf.matmul(valid_prediction, weights2) + biases2)  
  valid_prediction = tf.nn.relu(tf.matmul(valid_prediction,weights3)+biases3)
  valid_prediction = tf.nn.relu(tf.matmul(valid_prediction,weights4)+biases4)
  valid_prediction = tf.nn.softmax(tf.matmul(valid_prediction,weights5)+biases5)
  
  test_prediction = tf.nn.relu(tf.matmul(tf_test_dataset, weights) + biases)
  test_prediction = tf.nn.relu(tf.matmul(test_prediction, weights2) + biases2)  
  test_prediction = tf.nn.relu(tf.matmul(test_prediction,weights3)+biases3)
  test_prediction = tf.nn.relu(tf.matmul(test_prediction,weights4)+biases4)
  test_prediction = tf.nn.softmax(tf.matmul(test_prediction,weights5)+biases5)


    



In [ ]:

    
num_steps = 6001

with tf.Session(graph=graph) as session:
  tf.global_variables_initializer().run()
  print("Initialized")
  for step in range(num_steps):
    offset = batch_size*step%(train_labels.shape[0] - batch_size)
    batch_data   = train_dataset[offset:(offset+batch_size),:]
    batch_labels = train_labels[offset:(offset+batch_size),:]
    # Prepare a dictionary telling the session where to feed the minibatch.
    # The key of the dictionary is the placeholder node of the graph to be fed,
    # and the value is the numpy array to feed to it.
    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}
    _,l,pred = session.run([optimizer,loss,train_prediction],feed_dict=feed_dict)
    if step%1000==0:
      #print("global step and rate ",session.run([global_step,learning_rate]))
      print("step ", step)
      print ("loss = ", l)
      print("Batch accuracy: ", accuracy(pred,batch_labels))
      print("validation acc: ", accuracy(valid_prediction.eval(), valid_labels))
    #global_step = tf.add(global_step,1)
  print("test ",accuracy(test_prediction.eval(),test_labels))


    



In [ ]:

    
 


    

