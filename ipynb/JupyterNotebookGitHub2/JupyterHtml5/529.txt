
In [2]:

    
import tensorflow as tf


    



In [3]:

    
x1 = tf.Variable(3, name="x1")
x2 = tf.Variable(6, name="x2")
fnc = x1*2*x2 + x2


    



In [4]:

    
session = tf.Session()
session.run(x1.initializer)
session.run(x2.initializer)
result = session.run(fnc)
print (result)


    



In [5]:

    
session.close()


    



In [6]:

    
with tf.Session() as session:
    x1.initializer.run()
    x2.initializer.run()
    result=fnc.eval()
    print(result)


    



In [7]:

    
init = tf.global_variables_initializer()

with tf.Session() as session:
    init.run()
    result = fnc.eval()


    



In [8]:

    
y1 = tf.placeholder(tf.float32)
y2 = tf.placeholder(tf.float32)


    



In [9]:

    
sum_op = tf.add(y1, y2)
product_op = tf.multiply(y1, y2)


    



In [10]:

    
with tf.Session() as session:
    sum_result = session.run(sum_op, feed_dict={y1: 36.0, y2: 6.0})
    product_result = session.run(product_op, feed_dict={y1: 6.0, y2: 21.0})


    



In [11]:

    
print (sum_result)
print (product_result)


    



In [12]:

    
with tf.Session() as session:
    sum_result = session.run(sum_op, feed_dict={y1: [6.0, 4.0, 2.0], y2: [3, 2, 1.0]})
    product_result = session.run(product_op, feed_dict={y1: [2.0, 4.0], y2: 0.5})


    



In [13]:

    
print (sum_result)
print (product_result)


    



In [14]:

    
from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets("/tmp/data/", one_hot = True) # replace "/tmp/data/" with a folder on your system


    



In [44]:

    
import numpy as np


    



In [16]:

    
mnist.train.images[0].shape


    



In [15]:

    
x = tf.placeholder('float', [None, 784])
y = tf.placeholder('float')


    



In [60]:

    
def cnn_model(x, classes=10):
    # first reshape the input to a 2d image
    x = tf.reshape(x, shape=[-1, 28, 28, 1])
    
    w = tf.Variable(tf.random_normal([5,5,1,8])) # [filter_height, filter_width, in_channels, out_channels]
    conv1 = tf.nn.conv2d(x, w, strides=[1,1,1,1], padding='SAME')
    conv1 = tf.nn.relu(conv1)
    
    w = tf.Variable(tf.random_normal([3,3,8,8])) # [filter_height, filter_width, in_channels, out_channels]
    conv2 = tf.nn.conv2d(conv1, w, strides=[1,1,1,1], padding='SAME')
    conv2 = tf.nn.relu(conv2)
    
    # we need to flatten / reshape the output of the cnn
    w = tf.Variable(tf.random_normal([28*28*8,256]))
    bias = tf.Variable(tf.random_normal([256]))
    fc = tf.reshape(conv2, [-1, 8*28*28])
    fc = tf.matmul(fc, w)
    fc = fc + bias
    fc = tf.nn.relu(fc)
    
    w = tf.Variable(tf.random_normal([256, classes]))
    bias = tf.Variable(tf.random_normal([classes]))
    output = tf.matmul(fc, w) + bias
    # softmax activation will be done by softmax_cross_entropy_with_logits_v2
    
    return output


    



In [61]:

    
def train(x, y, model, epochs=10, batch_size=128):
    cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits_v2(logits=model, labels=y) )
    optimizer = tf.train.AdamOptimizer().minimize(cost)
    
    with tf.Session() as sess:
        sess.run(tf.initializers.global_variables())
        
        for epoch in range(epochs):
            epoch_loss = 0
            for batch in range(mnist.train.num_examples//batch_size):
                x_train, y_train = mnist.train.next_batch(batch_size)
                _, c = sess.run([optimizer, cost], feed_dict={x:x_train, y:y_train})
                epoch_loss += c
            
            print("Epoch %d / %d completed. Loss: %.3f" % (epoch+1, epochs, epoch_loss))
        
        # compute accuracy:
        correct = tf.equal(tf.argmax(model, 1), tf.argmax(y, 1))

        accuracy = tf.reduce_mean(tf.cast(correct, 'float'))
        print('Accuracy:',accuracy.eval({x:mnist.test.images, y:mnist.test.labels}))


    



In [65]:

    
model = cnn_model(x)
%time train(x, y, model)


    



In [66]:

    
def cnn_model_pooling(x, classes=10):
    # first reshape the input to a 2d image
    x = tf.reshape(x, shape=[-1, 28, 28, 1])
    
    w = tf.Variable(tf.random_normal([5,5,1,8])) # [filter_height, filter_width, in_channels, out_channels]
    conv1 = tf.nn.conv2d(x, w, strides=[1,1,1,1], padding='SAME')
    conv1 = tf.nn.relu(conv1)
    
    pool1 = tf.nn.max_pool(conv1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')
    
    w = tf.Variable(tf.random_normal([3,3,8,8])) # [filter_height, filter_width, in_channels, out_channels]
    conv2 = tf.nn.conv2d(pool1, w, strides=[1,1,1,1], padding='SAME')
    conv2 = tf.nn.relu(conv2)
    
    pool2 = tf.nn.max_pool(conv2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')
    
    # we need to flatten / reshape the output of the cnn
    # due to the max-pooling operations the image size reduced to 28/2/2 = 7
    w = tf.Variable(tf.random_normal([7*7*8,256]))
    bias = tf.Variable(tf.random_normal([256]))
    fc = tf.reshape(pool2, [-1, 8*7*7])
    fc = tf.matmul(fc, w)
    fc = fc + bias
    fc = tf.nn.relu(fc)
    
    w = tf.Variable(tf.random_normal([256, classes]))
    bias = tf.Variable(tf.random_normal([classes]))
    output = tf.matmul(fc, w) + bias
    # softmax activation will be done by softmax_cross_entropy_with_logits_v2
    
    return output


    



In [67]:

    
model = cnn_model_pooling(x)
%time train(x, y, model)


    



In [107]:

    
def inception(x, filers_per_conv=8):
    in_channels = int(x.shape[3])
    
    w = tf.Variable(tf.random_normal([1,1,in_channels,filers_per_conv]))
    conv1x1 = tf.nn.conv2d(x, w, strides=[1,1,1,1], padding='SAME')
    
    w = tf.Variable(tf.random_normal([3,3,in_channels,filers_per_conv]))
    conv3x3 = tf.nn.conv2d(x, w, strides=[1,1,1,1], padding='SAME')
    
    w = tf.Variable(tf.random_normal([5,5,in_channels,filers_per_conv]))
    conv5x5 = tf.nn.conv2d(x, w, strides=[1,1,1,1], padding='SAME')
    
    pool = tf.nn.max_pool(x, ksize=[1,3,3,1], strides=[1,1,1,1], padding='SAME')
    
    out = tf.concat([conv1x1, conv3x3, conv5x5, pool], 3)
    return out


    



In [111]:

    
def dim_reduction1x1conv(x, out_dim):
    # [filter_height, filter_width, in_channels, out_channels]
    w = tf.Variable(tf.random_normal([1,1,int(x.shape[3]),out_dim])) 
    conv = tf.nn.conv2d(x, w, strides=[1,1,1,1], padding='SAME')
    conv = tf.nn.relu(conv)
    return conv


    



In [112]:

    
def cnn_model_inception(x, classes=10):
    # first reshape the input to a 2d image
    x = tf.reshape(x, shape=[-1, 28, 28, 1])
    
    conv = tf.nn.relu(inception(x, 8))
    conv = dim_reduction1x1conv(conv, 8)
    conv = tf.nn.relu(inception(conv, 16))
    conv = dim_reduction1x1conv(conv, 16)
    
    # we need to flatten / reshape the output of the cnn
    w = tf.Variable(tf.random_normal([int(np.prod(conv.shape[1:])),512]))
    bias = tf.Variable(tf.random_normal([512]))
    fc = tf.reshape(conv, [-1, np.prod(conv.shape[1:])])
    fc = tf.matmul(fc, w)
    fc = fc + bias
    fc = tf.nn.relu(fc)
    
    w = tf.Variable(tf.random_normal([512, classes]))
    bias = tf.Variable(tf.random_normal([classes]))
    output = tf.matmul(fc, w) + bias
    # softmax activation will be done by softmax_cross_entropy_with_logits_v2
    
    return output


    



In [113]:

    
model = cnn_model_inception(x)
%time train(x, y, model)


    

