
In [26]:

    
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.examples.tutorials.mnist import input_data
# data
mnist = input_data.read_data_sets('MNIST_data', one_hot=True)

# assumes that input data is a row vector([1, 2, 3 ...])
def add_layer(data_input, num_inputs, num_neurons, activation_function=None, keep_prob=1.0,
              has_dims=False):
    weights = tf.Variable(tf.random_normal([num_neurons, num_inputs])) 
    biases = tf.Variable(tf.random_normal([num_neurons, 1]))  # one bias per neuron
    output = tf.add(tf.matmul(weights, data_input), biases)   # broadcast addition
    dropout_output = tf.nn.dropout(output, keep_prob)
    if activation_function:
        if has_dims == True:
            return activation_function(output, dim=0)
        return activation_function(output)
    return output


    



In [27]:

    
# define placeholder for the data
X_input = tf.placeholder(tf.float32, [784, None])
Y_input = tf.placeholder(tf.float32, [10, None])
dropout_prob = tf.placeholder(tf.float32)
learning_rate = 0.01
activation_function1, activation_function2 = tf.nn.tanh, tf.nn.softmax
num_neurons_h1, num_neurons_h2 = 10, 10
hidden_layer_1 = add_layer(X_input, 784, num_neurons_h1, activation_function1, dropout_prob)
prediction = add_layer(hidden_layer_1, num_neurons_h1, num_neurons_h2,
                                       activation_function2, dropout_prob, has_dims=True)
# using cross entropy loss -> tf does a sum to reduce the loss vector to a value
loss = -1 * tf.reduce_sum(Y_input * tf.log(prediction), 0)
train_func = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)


    



In [29]:

    
with tf.Session() as session:
    init = tf.global_variables_initializer()
    session.run(init)
    batch_size = 100
    iterations = 200
    batch_input, batch_output = mnist.train.next_batch(batch_size=batch_size)
    batch_input, batch_output = np.transpose(batch_input), np.transpose(batch_output)
    test_images, test_labels = np.transpose(mnist.test.images), np.transpose(mnist.test.labels)
    for iteration in range(iterations):
        session.run(train_func, feed_dict={X_input: batch_input,
                                           Y_input: batch_output, dropout_prob: 1})
        if iteration % 10 == 0:
            current_pred = session.run(prediction, feed_dict={
                                                    X_input: test_images, dropout_prob: 1.0})
            accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(current_pred, 0),
                                        tf.argmax(test_labels, 0)), tf.float32))
            print(session.run(accuracy))
            


    



In [ ]:

    
with tf.Session() as session:
    x = tf.nn.softmax(np.array([[1, 2, 3], [4, 5, 6]], dtype=float), dim=0)
    print(session.run(x))
    


    

