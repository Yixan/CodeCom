
In [1]:

    
def plot_image(some_image):
    
    some_digit_image = some_image.values.reshape(28,28)

    plt.imshow(some_digit_image, cmap = matplotlib.cm.binary, interpolation = "nearest")
    plt.axis("off")
    plt.show()


    



In [2]:

    
def get_label_name(idx):
    
    if (idx == 1):
        return '(1) TUMOR'
    elif (idx == 2):
        return '(2) STROMA'
    elif (idx == 3):
        return '(3) COMPLEX'
    elif (idx == 4):
        return '(4) LYMPHO'
    elif (idx == 5):
        return '(5) DEBRIS'
    elif (idx == 6):
        return '(6) MUCOSA'
    elif (idx == 7):
        return '(7) ADIPOSE'
    elif (idx == 8):
        return '(8) EMPTY'


    



In [3]:

    
%matplotlib inline
from glob import glob
import os
import matplotlib.pyplot as plt
import matplotlib
import numpy as np
import pandas as pd
from random import *


    



In [4]:

    
data = pd.read_csv('data/HTCP_28_28_L.csv')


    



In [5]:

    
yinput = data['label']
Xinput = data.drop(['label'], axis = 1)


    



In [6]:

    
Xinput.head()


    



In [7]:

    
yinput.head()


    



In [8]:

    
Xinput.shape


    



In [9]:

    
yinput.shape


    



In [10]:

    
yinput.unique()


    



In [11]:

    
def get_random_element_with_label (Xinput, lbls, lbl):
    tmp = lbls == lbl
    subset = Xinput[tmp]
    return subset.iloc[randint(1,subset.shape[0])]

labels_overview = np.empty([10,784])
for i in range (1,9):
    img = get_random_element_with_label(Xinput, yinput, i)
    labels_overview[i,:] = img


    



In [12]:

    
f = plt.figure(figsize=(8,15));
count = 1
for i in range(1,9):
    plt.subplot(5,2,count)
    count = count + 1
    plt.subplots_adjust(hspace=0.2)
    plt.title(get_label_name(i))
    some_digit_image = labels_overview[i,:].reshape(28,28)
    plt.imshow(some_digit_image, cmap = matplotlib.cm.binary, interpolation = "nearest")
    plt.axis("off")
    pass


    



In [13]:

    
total = 0
for i in range(9):
    print ("image", i, "appear", np.count_nonzero(yinput == i), "times")


    



In [14]:

    
total = 0
for i in range(9):
    print ("image", i, "makes", np.around(np.count_nonzero(yinput == i)/5000.0*100.0, decimals=1), "% of the 5000 observations")


    



In [15]:

    
from sklearn.model_selection import train_test_split
xtrain, xtest, ylabels, ylabels_test = train_test_split(Xinput, yinput, test_size=0.2, stratify=yinput, random_state=2018)


    



In [16]:

    
train = xtrain.transpose()
labels = ylabels.values.reshape(1, 4000)-1
labels_ = np.zeros((4000, 9))
labels_[np.arange(4000), labels] = 1
labels_ = labels_.transpose()


    



In [17]:

    
print(train.shape)
print(labels_.shape)


    



In [18]:

    
test = xtest.transpose()
labels_test = ylabels_test.values.reshape(1, 1000)-1
labels_test_ = np.zeros((1000, 9))
labels_test_[np.arange(1000), labels_test] = 1
labels_test_ = labels_test_.transpose()


    



In [19]:

    
print(labels_test_.shape)
print(test.shape)


    



In [20]:

    
train = np.array(train / 255.0)
test = np.array(test / 255.0)
labels_ = np.array(labels_)
labels_test_ = np.array(labels_test_)


    



In [21]:

    
import tensorflow as tf


    



In [22]:

    
def create_layer (X, n, activation):
    ndim = int(X.shape[0])
    stddev = 2.0 / np.sqrt(ndim)
    initialization = tf.truncated_normal((n, ndim), stddev = stddev)
    W = tf.Variable(initialization)
    b = tf.Variable(tf.zeros([n,1]))
    Z = tf.matmul(W,X)+b
    return activation(Z)


    



In [23]:

    
tf.reset_default_graph() 

n_dim = 784
n1 = 1
n_outputs = 9

tf.set_random_seed(5)

X = tf.placeholder(tf.float32, [n_dim, None])
Y = tf.placeholder(tf.float32, [9, None])

learning_rate = tf.placeholder(tf.float32, shape=())

hidden1 = create_layer (X, n1, activation = tf.nn.relu)
outputs = create_layer (hidden1, n_outputs, activation = tf.identity)
y_ = tf.nn.softmax(outputs)

cost = - tf.reduce_mean(Y * tf.log(y_)+(1-Y) * tf.log(1-y_))

optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)


    



In [24]:

    
def model(minibatch_size, training_epochs, features, classes, logging_step, learning_r):
    sess = tf.Session()
    sess.run(tf.global_variables_initializer())

    cost_history = []
    for epoch in range(training_epochs+1):
        for i in range(0, features.shape[1], minibatch_size):
            X_train_mini = features[:,i:i + minibatch_size]
            y_train_mini = classes[:,i:i + minibatch_size]

            sess.run(optimizer, feed_dict = {X: X_train_mini, Y: y_train_mini, learning_rate: learning_r})
        cost_ = sess.run(cost, feed_dict={ X:features, Y: classes, learning_rate: learning_r})
        cost_history = np.append(cost_history, cost_)

        if (epoch % logging_step == 0):
                print("Reached epoch",epoch,"cost J =", cost_)
                
    return sess, cost_history


    



In [25]:

    
sess, cost_history = model (50, 100, train, labels_,logging_step = 10, learning_r = 0.01)


    



In [26]:

    
correct_predictions = tf.equal(tf.argmax(y_,0), tf.argmax(Y,0))
accuracy = tf.reduce_mean(tf.cast(correct_predictions, "float"))
print ("Accuracy:", accuracy.eval({X: train, Y: labels_, learning_rate: 0.001}, session = sess))


    



In [27]:

    
correct_predictions = tf.equal(tf.argmax(y_,0), tf.argmax(Y,0))
accuracy = tf.reduce_mean(tf.cast(correct_predictions, "float"))
print ("Accuracy:", accuracy.eval({X: test, Y: labels_test_, learning_rate: 0.001}, session = sess))


    



In [28]:

    
tf.reset_default_graph() 

n_dim = 784
n1 = 10
n_outputs = 9

tf.set_random_seed(5)

X = tf.placeholder(tf.float32, [n_dim, None])
Y = tf.placeholder(tf.float32, [9, None])

learning_rate = tf.placeholder(tf.float32, shape=())

hidden1 = create_layer (X, n1, activation = tf.nn.relu)
outputs = create_layer (hidden1, n_outputs, activation = tf.identity)
y_ = tf.nn.softmax(outputs)

cost = - tf.reduce_mean(Y * tf.log(y_)+(1-Y) * tf.log(1-y_))

optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)


    



In [29]:

    
def model_layers(minibatch_size, training_epochs, features, classes, logging_step, 
                 learning_r, number_neurons, debug = False):
    
    opt, c, y_, X, Y, learning_rate = build_model_layers(number_neurons)
    
    sess = tf.Session()
    sess.run(tf.global_variables_initializer())

    cost_history = []
    for epoch in range(training_epochs+1):
        for i in range(0, features.shape[1], minibatch_size):
            X_train_mini = features[:,i:i + minibatch_size]
            y_train_mini = classes[:,i:i + minibatch_size]

            #if (i % 5000 == 0):
            #    print('i = ',i)
            
            sess.run(opt, feed_dict = {X: X_train_mini, Y: y_train_mini, learning_rate: learning_r})
        cost_ = sess.run(c, feed_dict={ X:features, Y: classes, learning_rate: learning_r})
        cost_history = np.append(cost_history, cost_)

        if ((epoch % logging_step == 0) & debug):
                print("Reached epoch",epoch,"cost J =", cost_)
                
    correct_predictions = tf.equal(tf.argmax(y_,0), tf.argmax(Y,0))
    accuracy = tf.reduce_mean(tf.cast(correct_predictions, "float"))
    accuracy_train = accuracy.eval({X: train, Y: labels_}, session = sess)
    accuracy_test = accuracy.eval({X: test, Y: labels_test_}, session = sess)
                
    return accuracy_train, accuracy_test, sess, cost_history


    



In [30]:

    
sess, cost_history = model (50, 100, train, labels_,logging_step = 10, learning_r = 0.01)


    



In [31]:

    
correct_predictions = tf.equal(tf.argmax(y_,0), tf.argmax(Y,0))
accuracy = tf.reduce_mean(tf.cast(correct_predictions, "float"))
print ("Accuracy:", accuracy.eval({X: train, Y: labels_, learning_rate: 0.001}, session = sess))


    



In [32]:

    
correct_predictions = tf.equal(tf.argmax(y_,0), tf.argmax(Y,0))
accuracy = tf.reduce_mean(tf.cast(correct_predictions, "float"))
print ("Accuracy:", accuracy.eval({X: test, Y: labels_test_, learning_rate: 0.001}, session = sess))


    



In [33]:

    
tf.reset_default_graph() 

n_dim = 784
n1 = 100
n_outputs = 9

tf.set_random_seed(5)

X = tf.placeholder(tf.float32, [n_dim, None])
Y = tf.placeholder(tf.float32, [9, None])

learning_rate = tf.placeholder(tf.float32, shape=())

hidden1 = create_layer (X, n1, activation = tf.nn.relu)
outputs = create_layer (hidden1, n_outputs, activation = tf.identity)
y_ = tf.nn.softmax(outputs)

cost = - tf.reduce_mean(Y * tf.log(y_)+(1-Y) * tf.log(1-y_))

optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)


    



In [34]:

    
def model_layers(minibatch_size, training_epochs, features, classes, logging_step, 
                 learning_r, number_neurons, debug = False):
    
    opt, c, y_, X, Y, learning_rate = build_model_layers(number_neurons)
    
    sess = tf.Session()
    sess.run(tf.global_variables_initializer())

    cost_history = []
    for epoch in range(training_epochs+1):
        for i in range(0, features.shape[1], minibatch_size):
            X_train_mini = features[:,i:i + minibatch_size]
            y_train_mini = classes[:,i:i + minibatch_size]

            #if (i % 5000 == 0):
            #    print('i = ',i)
            
            sess.run(opt, feed_dict = {X: X_train_mini, Y: y_train_mini, learning_rate: learning_r})
        cost_ = sess.run(c, feed_dict={ X:features, Y: classes, learning_rate: learning_r})
        cost_history = np.append(cost_history, cost_)

        if ((epoch % logging_step == 0) & debug):
                print("Reached epoch",epoch,"cost J =", cost_)
                
    correct_predictions = tf.equal(tf.argmax(y_,0), tf.argmax(Y,0))
    accuracy = tf.reduce_mean(tf.cast(correct_predictions, "float"))
    accuracy_train = accuracy.eval({X: train, Y: labels_}, session = sess)
    accuracy_test = accuracy.eval({X: test, Y: labels_test_}, session = sess)
                
    return accuracy_train, accuracy_test, sess, cost_history


    



In [35]:

    
sess, cost_history = model (50, 100, train, labels_,logging_step = 10, learning_r = 0.01)


    



In [36]:

    
correct_predictions = tf.equal(tf.argmax(y_,0), tf.argmax(Y,0))
accuracy = tf.reduce_mean(tf.cast(correct_predictions, "float"))
print ("Accuracy:", accuracy.eval({X: train, Y: labels_, learning_rate: 0.001}, session = sess))


    



In [37]:

    
correct_predictions = tf.equal(tf.argmax(y_,0), tf.argmax(Y,0))
accuracy = tf.reduce_mean(tf.cast(correct_predictions, "float"))
print ("Accuracy:", accuracy.eval({X: test, Y: labels_test_, learning_rate: 0.001}, session = sess))


    



In [38]:

    
tf.reset_default_graph() 

n_dim = 784
n1 = 100
n2 = 100
n_outputs = 9

tf.set_random_seed(5)

X = tf.placeholder(tf.float32, [n_dim, None])
Y = tf.placeholder(tf.float32, [9, None])

learning_rate = tf.placeholder(tf.float32, shape=())

hidden1 = create_layer (X, n1, activation = tf.nn.relu)
hidden2 = create_layer (hidden1, n2, activation = tf.nn.relu)
outputs = create_layer (hidden2, n_outputs, activation = tf.identity)
y_ = tf.nn.softmax(outputs)

cost = - tf.reduce_mean(Y * tf.log(y_)+(1-Y) * tf.log(1-y_))

optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)


    



In [39]:

    
def model_layers(minibatch_size, training_epochs, features, classes, logging_step, 
                 learning_r, number_neurons, debug = False):
    
    opt, c, y_, X, Y, learning_rate = build_model_layers(number_neurons)
    
    sess = tf.Session()
    sess.run(tf.global_variables_initializer())

    cost_history = []
    for epoch in range(training_epochs+1):
        for i in range(0, features.shape[1], minibatch_size):
            X_train_mini = features[:,i:i + minibatch_size]
            y_train_mini = classes[:,i:i + minibatch_size]

            #if (i % 5000 == 0):
            #    print('i = ',i)
            
            sess.run(opt, feed_dict = {X: X_train_mini, Y: y_train_mini, learning_rate: learning_r})
        cost_ = sess.run(c, feed_dict={ X:features, Y: classes, learning_rate: learning_r})
        cost_history = np.append(cost_history, cost_)

        if ((epoch % logging_step == 0) & debug):
                print("Reached epoch",epoch,"cost J =", cost_)
                
    correct_predictions = tf.equal(tf.argmax(y_,0), tf.argmax(Y,0))
    accuracy = tf.reduce_mean(tf.cast(correct_predictions, "float"))
    accuracy_train = accuracy.eval({X: train, Y: labels_}, session = sess)
    accuracy_test = accuracy.eval({X: test, Y: labels_test_}, session = sess)
                
    return accuracy_train, accuracy_test, sess, cost_history


    



In [40]:

    
sess, cost_history = model (50, 100, train, labels_,logging_step = 10, learning_r = 0.01)


    



In [41]:

    
correct_predictions = tf.equal(tf.argmax(y_,0), tf.argmax(Y,0))
accuracy = tf.reduce_mean(tf.cast(correct_predictions, "float"))
print ("Accuracy:", accuracy.eval({X: train, Y: labels_, learning_rate: 0.001}, session = sess))


    



In [42]:

    
correct_predictions = tf.equal(tf.argmax(y_,0), tf.argmax(Y,0))
accuracy = tf.reduce_mean(tf.cast(correct_predictions, "float"))
print ("Accuracy:", accuracy.eval({X: test, Y: labels_test_, learning_rate: 0.001}, session = sess))


    



In [43]:

    
tf.reset_default_graph() 

n_dim = 784
n1 = 100
n2 = 100
n3 = 100
n_outputs = 9

tf.set_random_seed(5)

X = tf.placeholder(tf.float32, [n_dim, None])
Y = tf.placeholder(tf.float32, [9, None])

learning_rate = tf.placeholder(tf.float32, shape=())

hidden1 = create_layer (X, n1, activation = tf.nn.relu)
hidden2 = create_layer (hidden1, n2, activation = tf.nn.relu)
hidden3 = create_layer (hidden2, n3, activation = tf.nn.relu)
outputs = create_layer (hidden3, n_outputs, activation = tf.identity)
y_ = tf.nn.softmax(outputs)

cost = - tf.reduce_mean(Y * tf.log(y_)+(1-Y) * tf.log(1-y_))

optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)


    



In [44]:

    
def model_layers(minibatch_size, training_epochs, features, classes, logging_step, 
                 learning_r, number_neurons, debug = False):
    
    opt, c, y_, X, Y, learning_rate = build_model_layers(number_neurons)
    
    sess = tf.Session()
    sess.run(tf.global_variables_initializer())

    cost_history = []
    for epoch in range(training_epochs+1):
        for i in range(0, features.shape[1], minibatch_size):
            X_train_mini = features[:,i:i + minibatch_size]
            y_train_mini = classes[:,i:i + minibatch_size]

            #if (i % 5000 == 0):
            #    print('i = ',i)
            
            sess.run(opt, feed_dict = {X: X_train_mini, Y: y_train_mini, learning_rate: learning_r})
        cost_ = sess.run(c, feed_dict={ X:features, Y: classes, learning_rate: learning_r})
        cost_history = np.append(cost_history, cost_)

        if ((epoch % logging_step == 0) & debug):
                print("Reached epoch",epoch,"cost J =", cost_)
                
    correct_predictions = tf.equal(tf.argmax(y_,0), tf.argmax(Y,0))
    accuracy = tf.reduce_mean(tf.cast(correct_predictions, "float"))
    accuracy_train = accuracy.eval({X: train, Y: labels_}, session = sess)
    accuracy_test = accuracy.eval({X: test, Y: labels_test_}, session = sess)
                
    return accuracy_train, accuracy_test, sess, cost_history


    



In [45]:

    
sess, cost_history = model (50, 100, train, labels_,logging_step = 10, learning_r = 0.01)


    



In [46]:

    
correct_predictions = tf.equal(tf.argmax(y_,0), tf.argmax(Y,0))
accuracy = tf.reduce_mean(tf.cast(correct_predictions, "float"))
print ("Accuracy:", accuracy.eval({X: train, Y: labels_, learning_rate: 0.001}, session = sess))


    



In [47]:

    
correct_predictions = tf.equal(tf.argmax(y_,0), tf.argmax(Y,0))
accuracy = tf.reduce_mean(tf.cast(correct_predictions, "float"))
print ("Accuracy:", accuracy.eval({X: test, Y: labels_test_, learning_rate: 0.001}, session = sess))


    



In [48]:

    
tf.reset_default_graph() 

n_dim = 784
n1 = 100
n2 = 100
n3 = 100
n4 = 100
n_outputs = 9

tf.set_random_seed(5)

X = tf.placeholder(tf.float32, [n_dim, None])
Y = tf.placeholder(tf.float32, [9, None])

learning_rate = tf.placeholder(tf.float32, shape=())

hidden1 = create_layer (X, n1, activation = tf.nn.relu)
hidden2 = create_layer (hidden1, n2, activation = tf.nn.relu)
hidden3 = create_layer (hidden2, n3, activation = tf.nn.relu)
hidden4 = create_layer (hidden3, n4, activation = tf.nn.relu)
outputs = create_layer (hidden4, n_outputs, activation = tf.identity)
y_ = tf.nn.softmax(outputs)

cost = - tf.reduce_mean(Y * tf.log(y_)+(1-Y) * tf.log(1-y_))

optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)


    



In [49]:

    
def model_layers(minibatch_size, training_epochs, features, classes, logging_step, 
                 learning_r, number_neurons, debug = False):
    
    opt, c, y_, X, Y, learning_rate = build_model_layers(number_neurons)
    
    sess = tf.Session()
    sess.run(tf.global_variables_initializer())

    cost_history = []
    for epoch in range(training_epochs+1):
        for i in range(0, features.shape[1], minibatch_size):
            X_train_mini = features[:,i:i + minibatch_size]
            y_train_mini = classes[:,i:i + minibatch_size]

            #if (i % 5000 == 0):
            #    print('i = ',i)
            
            sess.run(opt, feed_dict = {X: X_train_mini, Y: y_train_mini, learning_rate: learning_r})
        cost_ = sess.run(c, feed_dict={ X:features, Y: classes, learning_rate: learning_r})
        cost_history = np.append(cost_history, cost_)

        if ((epoch % logging_step == 0) & debug):
                print("Reached epoch",epoch,"cost J =", cost_)
                
    correct_predictions = tf.equal(tf.argmax(y_,0), tf.argmax(Y,0))
    accuracy = tf.reduce_mean(tf.cast(correct_predictions, "float"))
    accuracy_train = accuracy.eval({X: train, Y: labels_}, session = sess)
    accuracy_test = accuracy.eval({X: test, Y: labels_test_}, session = sess)
                
    return accuracy_train, accuracy_test, sess, cost_history


    



In [50]:

    
sess, cost_history = model (50, 100, train, labels_,logging_step = 10, learning_r = 0.01)


    



In [51]:

    
correct_predictions = tf.equal(tf.argmax(y_,0), tf.argmax(Y,0))
accuracy = tf.reduce_mean(tf.cast(correct_predictions, "float"))
print ("Accuracy:", accuracy.eval({X: train, Y: labels_, learning_rate: 0.001}, session = sess))


    



In [52]:

    
correct_predictions = tf.equal(tf.argmax(y_,0), tf.argmax(Y,0))
accuracy = tf.reduce_mean(tf.cast(correct_predictions, "float"))
print ("Accuracy:", accuracy.eval({X: test, Y: labels_test_, learning_rate: 0.001}, session = sess))


    



In [53]:

    
tf.reset_default_graph() 

n_dim = 784
n1 = 100
n2 = 100
n3 = 100
n4 = 100
n5 = 100
n_outputs = 9

tf.set_random_seed(5)

X = tf.placeholder(tf.float32, [n_dim, None])
Y = tf.placeholder(tf.float32, [9, None])

learning_rate = tf.placeholder(tf.float32, shape=())

hidden1 = create_layer (X, n1, activation = tf.nn.relu)
hidden2 = create_layer (hidden1, n2, activation = tf.nn.relu)
hidden3 = create_layer (hidden2, n3, activation = tf.nn.relu)
hidden4 = create_layer (hidden3, n4, activation = tf.nn.relu)
hidden5 = create_layer (hidden4, n5, activation = tf.nn.relu)
outputs = create_layer (hidden5, n_outputs, activation = tf.identity)
y_ = tf.nn.softmax(outputs)

cost = - tf.reduce_mean(Y * tf.log(y_)+(1-Y) * tf.log(1-y_))

optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)


    



In [54]:

    
def model_layers(minibatch_size, training_epochs, features, classes, logging_step, 
                 learning_r, number_neurons, debug = False):
    
    opt, c, y_, X, Y, learning_rate = build_model_layers(number_neurons)
    
    sess = tf.Session()
    sess.run(tf.global_variables_initializer())

    cost_history = []
    for epoch in range(training_epochs+1):
        for i in range(0, features.shape[1], minibatch_size):
            X_train_mini = features[:,i:i + minibatch_size]
            y_train_mini = classes[:,i:i + minibatch_size]

            #if (i % 5000 == 0):
            #    print('i = ',i)
            
            sess.run(opt, feed_dict = {X: X_train_mini, Y: y_train_mini, learning_rate: learning_r})
        cost_ = sess.run(c, feed_dict={ X:features, Y: classes, learning_rate: learning_r})
        cost_history = np.append(cost_history, cost_)

        if ((epoch % logging_step == 0) & debug):
                print("Reached epoch",epoch,"cost J =", cost_)
                
    correct_predictions = tf.equal(tf.argmax(y_,0), tf.argmax(Y,0))
    accuracy = tf.reduce_mean(tf.cast(correct_predictions, "float"))
    accuracy_train = accuracy.eval({X: train, Y: labels_}, session = sess)
    accuracy_test = accuracy.eval({X: test, Y: labels_test_}, session = sess)
                
    return accuracy_train, accuracy_test, sess, cost_history


    



In [55]:

    
sess, cost_history = model (50, 100, train, labels_,logging_step = 10, learning_r = 0.01)


    



In [56]:

    
correct_predictions = tf.equal(tf.argmax(y_,0), tf.argmax(Y,0))
accuracy = tf.reduce_mean(tf.cast(correct_predictions, "float"))
print ("Accuracy:", accuracy.eval({X: train, Y: labels_, learning_rate: 0.001}, session = sess))


    



In [57]:

    
correct_predictions = tf.equal(tf.argmax(y_,0), tf.argmax(Y,0))
accuracy = tf.reduce_mean(tf.cast(correct_predictions, "float"))
print ("Accuracy:", accuracy.eval({X: test, Y: labels_test_, learning_rate: 0.001}, session = sess))


    



In [58]:

    
tf.reset_default_graph() 

n_dim = 784
n1 = 100
n2 = 100
n3 = 100
n4 = 100
n5 = 100
n6 = 100
n_outputs = 9

tf.set_random_seed(5)

X = tf.placeholder(tf.float32, [n_dim, None])
Y = tf.placeholder(tf.float32, [9, None])

learning_rate = tf.placeholder(tf.float32, shape=())

hidden1 = create_layer (X, n1, activation = tf.nn.relu)
hidden2 = create_layer (hidden1, n2, activation = tf.nn.relu)
hidden3 = create_layer (hidden2, n3, activation = tf.nn.relu)
hidden4 = create_layer (hidden3, n4, activation = tf.nn.relu)
hidden5 = create_layer (hidden4, n5, activation = tf.nn.relu)
hidden6 = create_layer (hidden5, n6, activation = tf.nn.relu)
outputs = create_layer (hidden6, n_outputs, activation = tf.identity)
y_ = tf.nn.softmax(outputs)

cost = - tf.reduce_mean(Y * tf.log(y_)+(1-Y) * tf.log(1-y_))

optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)


    



In [59]:

    
def model_layers(minibatch_size, training_epochs, features, classes, logging_step, 
                 learning_r, number_neurons, debug = False):
    
    opt, c, y_, X, Y, learning_rate = build_model_layers(number_neurons)
    
    sess = tf.Session()
    sess.run(tf.global_variables_initializer())

    cost_history = []
    for epoch in range(training_epochs+1):
        for i in range(0, features.shape[1], minibatch_size):
            X_train_mini = features[:,i:i + minibatch_size]
            y_train_mini = classes[:,i:i + minibatch_size]

            #if (i % 5000 == 0):
            #    print('i = ',i)
            
            sess.run(opt, feed_dict = {X: X_train_mini, Y: y_train_mini, learning_rate: learning_r})
        cost_ = sess.run(c, feed_dict={ X:features, Y: classes, learning_rate: learning_r})
        cost_history = np.append(cost_history, cost_)

        if ((epoch % logging_step == 0) & debug):
                print("Reached epoch",epoch,"cost J =", cost_)
                
    correct_predictions = tf.equal(tf.argmax(y_,0), tf.argmax(Y,0))
    accuracy = tf.reduce_mean(tf.cast(correct_predictions, "float"))
    accuracy_train = accuracy.eval({X: train, Y: labels_}, session = sess)
    accuracy_test = accuracy.eval({X: test, Y: labels_test_}, session = sess)
                
    return accuracy_train, accuracy_test, sess, cost_history


    



In [60]:

    
sess, cost_history = model (50, 100, train, labels_,logging_step = 10, learning_r = 0.01)


    



In [61]:

    
correct_predictions = tf.equal(tf.argmax(y_,0), tf.argmax(Y,0))
accuracy = tf.reduce_mean(tf.cast(correct_predictions, "float"))
print ("Accuracy:", accuracy.eval({X: train, Y: labels_, learning_rate: 0.001}, session = sess))


    



In [62]:

    
correct_predictions = tf.equal(tf.argmax(y_,0), tf.argmax(Y,0))
accuracy = tf.reduce_mean(tf.cast(correct_predictions, "float"))
print ("Accuracy:", accuracy.eval({X: test, Y: labels_test_, learning_rate: 0.001}, session = sess))


    



In [63]:

    
tf.reset_default_graph() 

n_dim = 784
n1 = 100
n2 = 100
n3 = 100
n4 = 100
n5 = 100
n6 = 100
n7 = 100
n_outputs = 9

tf.set_random_seed(5)

X = tf.placeholder(tf.float32, [n_dim, None])
Y = tf.placeholder(tf.float32, [9, None])

learning_rate = tf.placeholder(tf.float32, shape=())

hidden1 = create_layer (X, n1, activation = tf.nn.relu)
hidden2 = create_layer (hidden1, n2, activation = tf.nn.relu)
hidden3 = create_layer (hidden2, n3, activation = tf.nn.relu)
hidden4 = create_layer (hidden3, n4, activation = tf.nn.relu)
hidden5 = create_layer (hidden4, n5, activation = tf.nn.relu)
hidden6 = create_layer (hidden5, n6, activation = tf.nn.relu)
hidden7 = create_layer (hidden6, n7, activation = tf.nn.relu)
outputs = create_layer (hidden7, n_outputs, activation = tf.identity)
y_ = tf.nn.softmax(outputs)

cost = - tf.reduce_mean(Y * tf.log(y_)+(1-Y) * tf.log(1-y_))

optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)


    



In [64]:

    
def model_layers(minibatch_size, training_epochs, features, classes, logging_step, 
                 learning_r, number_neurons, debug = False):
    
    opt, c, y_, X, Y, learning_rate = build_model_layers(number_neurons)
    
    sess = tf.Session()
    sess.run(tf.global_variables_initializer())

    cost_history = []
    for epoch in range(training_epochs+1):
        for i in range(0, features.shape[1], minibatch_size):
            X_train_mini = features[:,i:i + minibatch_size]
            y_train_mini = classes[:,i:i + minibatch_size]

            #if (i % 5000 == 0):
            #    print('i = ',i)
            
            sess.run(opt, feed_dict = {X: X_train_mini, Y: y_train_mini, learning_rate: learning_r})
        cost_ = sess.run(c, feed_dict={ X:features, Y: classes, learning_rate: learning_r})
        cost_history = np.append(cost_history, cost_)

        if ((epoch % logging_step == 0) & debug):
                print("Reached epoch",epoch,"cost J =", cost_)
                
    correct_predictions = tf.equal(tf.argmax(y_,0), tf.argmax(Y,0))
    accuracy = tf.reduce_mean(tf.cast(correct_predictions, "float"))
    accuracy_train = accuracy.eval({X: train, Y: labels_}, session = sess)
    accuracy_test = accuracy.eval({X: test, Y: labels_test_}, session = sess)
                
    return accuracy_train, accuracy_test, sess, cost_history


    



In [65]:

    
sess, cost_history = model (50, 100, train, labels_,logging_step = 10, learning_r = 0.01)


    



In [66]:

    
correct_predictions = tf.equal(tf.argmax(y_,0), tf.argmax(Y,0))
accuracy = tf.reduce_mean(tf.cast(correct_predictions, "float"))
print ("Accuracy:", accuracy.eval({X: train, Y: labels_, learning_rate: 0.001}, session = sess))


    



In [67]:

    
correct_predictions = tf.equal(tf.argmax(y_,0), tf.argmax(Y,0))
accuracy = tf.reduce_mean(tf.cast(correct_predictions, "float"))
print ("Accuracy:", accuracy.eval({X: test, Y: labels_test_, learning_rate: 0.001}, session = sess))


    



In [68]:

    
tf.reset_default_graph() 

n_dim = 784
n1 = 100
n2 = 100
n3 = 100
n4 = 100
n5 = 100
n6 = 100
n7 = 100
n8 = 100
n_outputs = 9

tf.set_random_seed(5)

X = tf.placeholder(tf.float32, [n_dim, None])
Y = tf.placeholder(tf.float32, [9, None])

learning_rate = tf.placeholder(tf.float32, shape=())

hidden1 = create_layer (X, n1, activation = tf.nn.relu)
hidden2 = create_layer (hidden1, n2, activation = tf.nn.relu)
hidden3 = create_layer (hidden2, n3, activation = tf.nn.relu)
hidden4 = create_layer (hidden3, n4, activation = tf.nn.relu)
hidden5 = create_layer (hidden4, n5, activation = tf.nn.relu)
hidden6 = create_layer (hidden5, n6, activation = tf.nn.relu)
hidden7 = create_layer (hidden6, n7, activation = tf.nn.relu)
hidden8 = create_layer (hidden7, n8, activation = tf.nn.relu)
outputs = create_layer (hidden8, n_outputs, activation = tf.identity)
y_ = tf.nn.softmax(outputs)

cost = - tf.reduce_mean(Y * tf.log(y_)+(1-Y) * tf.log(1-y_))

optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)


    



In [69]:

    
def model_layers(minibatch_size, training_epochs, features, classes, logging_step, 
                 learning_r, number_neurons, debug = False):
    
    opt, c, y_, X, Y, learning_rate = build_model_layers(number_neurons)
    
    sess = tf.Session()
    sess.run(tf.global_variables_initializer())

    cost_history = []
    for epoch in range(training_epochs+1):
        for i in range(0, features.shape[1], minibatch_size):
            X_train_mini = features[:,i:i + minibatch_size]
            y_train_mini = classes[:,i:i + minibatch_size]

            #if (i % 5000 == 0):
            #    print('i = ',i)
            
            sess.run(opt, feed_dict = {X: X_train_mini, Y: y_train_mini, learning_rate: learning_r})
        cost_ = sess.run(c, feed_dict={ X:features, Y: classes, learning_rate: learning_r})
        cost_history = np.append(cost_history, cost_)

        if ((epoch % logging_step == 0) & debug):
                print("Reached epoch",epoch,"cost J =", cost_)
                
    correct_predictions = tf.equal(tf.argmax(y_,0), tf.argmax(Y,0))
    accuracy = tf.reduce_mean(tf.cast(correct_predictions, "float"))
    accuracy_train = accuracy.eval({X: train, Y: labels_}, session = sess)
    accuracy_test = accuracy.eval({X: test, Y: labels_test_}, session = sess)
                
    return accuracy_train, accuracy_test, sess, cost_history


    



In [70]:

    
sess, cost_history = model (50, 100, train, labels_,logging_step = 10, learning_r = 0.01)


    



In [71]:

    
correct_predictions = tf.equal(tf.argmax(y_,0), tf.argmax(Y,0))
accuracy = tf.reduce_mean(tf.cast(correct_predictions, "float"))
print ("Accuracy:", accuracy.eval({X: train, Y: labels_, learning_rate: 0.001}, session = sess))


    



In [72]:

    
correct_predictions = tf.equal(tf.argmax(y_,0), tf.argmax(Y,0))
accuracy = tf.reduce_mean(tf.cast(correct_predictions, "float"))
print ("Accuracy:", accuracy.eval({X: test, Y: labels_test_, learning_rate: 0.001}, session = sess))


    



In [73]:

    
tf.reset_default_graph() 

n_dim = 784
n1 = 100
n2 = 100
n3 = 100
n4 = 100
n5 = 100
n6 = 100
n7 = 100
n8 = 100
n9 = 100
n_outputs = 9

tf.set_random_seed(5)

X = tf.placeholder(tf.float32, [n_dim, None])
Y = tf.placeholder(tf.float32, [9, None])

learning_rate = tf.placeholder(tf.float32, shape=())

hidden1 = create_layer (X, n1, activation = tf.nn.relu)
hidden2 = create_layer (hidden1, n2, activation = tf.nn.relu)
hidden3 = create_layer (hidden2, n3, activation = tf.nn.relu)
hidden4 = create_layer (hidden3, n4, activation = tf.nn.relu)
hidden5 = create_layer (hidden4, n5, activation = tf.nn.relu)
hidden6 = create_layer (hidden5, n6, activation = tf.nn.relu)
hidden7 = create_layer (hidden6, n7, activation = tf.nn.relu)
hidden8 = create_layer (hidden7, n8, activation = tf.nn.relu)
hidden9 = create_layer (hidden8, n9, activation = tf.nn.relu)
outputs = create_layer (hidden9, n_outputs, activation = tf.identity)
y_ = tf.nn.softmax(outputs)

cost = - tf.reduce_mean(Y * tf.log(y_)+(1-Y) * tf.log(1-y_))

optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)


    



In [74]:

    
def model_layers(minibatch_size, training_epochs, features, classes, logging_step, 
                 learning_r, number_neurons, debug = False):
    
    opt, c, y_, X, Y, learning_rate = build_model_layers(number_neurons)
    
    sess = tf.Session()
    sess.run(tf.global_variables_initializer())

    cost_history = []
    for epoch in range(training_epochs+1):
        for i in range(0, features.shape[1], minibatch_size):
            X_train_mini = features[:,i:i + minibatch_size]
            y_train_mini = classes[:,i:i + minibatch_size]

            #if (i % 5000 == 0):
            #    print('i = ',i)
            
            sess.run(opt, feed_dict = {X: X_train_mini, Y: y_train_mini, learning_rate: learning_r})
        cost_ = sess.run(c, feed_dict={ X:features, Y: classes, learning_rate: learning_r})
        cost_history = np.append(cost_history, cost_)

        if ((epoch % logging_step == 0) & debug):
                print("Reached epoch",epoch,"cost J =", cost_)
                
    correct_predictions = tf.equal(tf.argmax(y_,0), tf.argmax(Y,0))
    accuracy = tf.reduce_mean(tf.cast(correct_predictions, "float"))
    accuracy_train = accuracy.eval({X: train, Y: labels_}, session = sess)
    accuracy_test = accuracy.eval({X: test, Y: labels_test_}, session = sess)
                
    return accuracy_train, accuracy_test, sess, cost_history


    



In [75]:

    
sess, cost_history = model (50, 100, train, labels_,logging_step = 10, learning_r = 0.01)


    



In [76]:

    
correct_predictions = tf.equal(tf.argmax(y_,0), tf.argmax(Y,0))
accuracy = tf.reduce_mean(tf.cast(correct_predictions, "float"))
print ("Accuracy:", accuracy.eval({X: train, Y: labels_, learning_rate: 0.001}, session = sess))


    



In [77]:

    
correct_predictions = tf.equal(tf.argmax(y_,0), tf.argmax(Y,0))
accuracy = tf.reduce_mean(tf.cast(correct_predictions, "float"))
print ("Accuracy:", accuracy.eval({X: test, Y: labels_test_, learning_rate: 0.001}, session = sess))


    



In [78]:

    
tf.reset_default_graph() 

n_dim = 784
n1 = 100
n2 = 100
n3 = 100
n4 = 100
n5 = 100
n6 = 100
n7 = 100
n8 = 100
n9 = 100
n10 = 100
n_outputs = 9

tf.set_random_seed(5)

X = tf.placeholder(tf.float32, [n_dim, None])
Y = tf.placeholder(tf.float32, [9, None])

learning_rate = tf.placeholder(tf.float32, shape=())

hidden1 = create_layer (X, n1, activation = tf.nn.relu)
hidden2 = create_layer (hidden1, n2, activation = tf.nn.relu)
hidden3 = create_layer (hidden2, n3, activation = tf.nn.relu)
hidden4 = create_layer (hidden3, n4, activation = tf.nn.relu)
hidden5 = create_layer (hidden4, n5, activation = tf.nn.relu)
hidden6 = create_layer (hidden5, n6, activation = tf.nn.relu)
hidden7 = create_layer (hidden6, n7, activation = tf.nn.relu)
hidden8 = create_layer (hidden7, n8, activation = tf.nn.relu)
hidden9 = create_layer (hidden8, n9, activation = tf.nn.relu)
hidden10 = create_layer (hidden9, n10, activation = tf.nn.relu)
outputs = create_layer (hidden10, n_outputs, activation = tf.identity)
y_ = tf.nn.softmax(outputs)

cost = - tf.reduce_mean(Y * tf.log(y_)+(1-Y) * tf.log(1-y_))

optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)


    



In [79]:

    
def model_layers(minibatch_size, training_epochs, features, classes, logging_step, 
                 learning_r, number_neurons, debug = False):
    
    opt, c, y_, X, Y, learning_rate = build_model_layers(number_neurons)
    
    sess = tf.Session()
    sess.run(tf.global_variables_initializer())

    cost_history = []
    for epoch in range(training_epochs+1):
        for i in range(0, features.shape[1], minibatch_size):
            X_train_mini = features[:,i:i + minibatch_size]
            y_train_mini = classes[:,i:i + minibatch_size]

            #if (i % 5000 == 0):
            #    print('i = ',i)
            
            sess.run(opt, feed_dict = {X: X_train_mini, Y: y_train_mini, learning_rate: learning_r})
        cost_ = sess.run(c, feed_dict={ X:features, Y: classes, learning_rate: learning_r})
        cost_history = np.append(cost_history, cost_)

        if ((epoch % logging_step == 0) & debug):
                print("Reached epoch",epoch,"cost J =", cost_)
                
    correct_predictions = tf.equal(tf.argmax(y_,0), tf.argmax(Y,0))
    accuracy = tf.reduce_mean(tf.cast(correct_predictions, "float"))
    accuracy_train = accuracy.eval({X: train, Y: labels_}, session = sess)
    accuracy_test = accuracy.eval({X: test, Y: labels_test_}, session = sess)
                
    return accuracy_train, accuracy_test, sess, cost_history


    



In [80]:

    
sess, cost_history = model (50, 100, train, labels_,logging_step = 10, learning_r = 0.01)


    



In [81]:

    
correct_predictions = tf.equal(tf.argmax(y_,0), tf.argmax(Y,0))
accuracy = tf.reduce_mean(tf.cast(correct_predictions, "float"))
print ("Accuracy:", accuracy.eval({X: train, Y: labels_, learning_rate: 0.001}, session = sess))


    



In [82]:

    
correct_predictions = tf.equal(tf.argmax(y_,0), tf.argmax(Y,0))
accuracy = tf.reduce_mean(tf.cast(correct_predictions, "float"))
print ("Accuracy:", accuracy.eval({X: test, Y: labels_test_, learning_rate: 0.001}, session = sess))


    



In [83]:

    
tf.reset_default_graph() 

n_dim = 784
n1 = 100
n2 = 100
n3 = 100
n4 = 100
n5 = 100
n6 = 100
n7 = 100
n8 = 100
n9 = 100
n10 = 100
n11 = 100
n_outputs = 9

tf.set_random_seed(5)

X = tf.placeholder(tf.float32, [n_dim, None])
Y = tf.placeholder(tf.float32, [9, None])

learning_rate = tf.placeholder(tf.float32, shape=())

hidden1 = create_layer (X, n1, activation = tf.nn.relu)
hidden2 = create_layer (hidden1, n2, activation = tf.nn.relu)
hidden3 = create_layer (hidden2, n3, activation = tf.nn.relu)
hidden4 = create_layer (hidden3, n4, activation = tf.nn.relu)
hidden5 = create_layer (hidden4, n5, activation = tf.nn.relu)
hidden6 = create_layer (hidden5, n6, activation = tf.nn.relu)
hidden7 = create_layer (hidden6, n7, activation = tf.nn.relu)
hidden8 = create_layer (hidden7, n8, activation = tf.nn.relu)
hidden9 = create_layer (hidden8, n9, activation = tf.nn.relu)
hidden10 = create_layer (hidden9, n10, activation = tf.nn.relu)
hidden11 = create_layer (hidden10, n11, activation = tf.nn.relu)
outputs = create_layer (hidden11, n_outputs, activation = tf.identity)
y_ = tf.nn.softmax(outputs)

cost = - tf.reduce_mean(Y * tf.log(y_)+(1-Y) * tf.log(1-y_))

optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)


    



In [84]:

    
def model_layers(minibatch_size, training_epochs, features, classes, logging_step, 
                 learning_r, number_neurons, debug = False):
    
    opt, c, y_, X, Y, learning_rate = build_model_layers(number_neurons)
    
    sess = tf.Session()
    sess.run(tf.global_variables_initializer())

    cost_history = []
    for epoch in range(training_epochs+1):
        for i in range(0, features.shape[1], minibatch_size):
            X_train_mini = features[:,i:i + minibatch_size]
            y_train_mini = classes[:,i:i + minibatch_size]

            #if (i % 5000 == 0):
            #    print('i = ',i)
            
            sess.run(opt, feed_dict = {X: X_train_mini, Y: y_train_mini, learning_rate: learning_r})
        cost_ = sess.run(c, feed_dict={ X:features, Y: classes, learning_rate: learning_r})
        cost_history = np.append(cost_history, cost_)

        if ((epoch % logging_step == 0) & debug):
                print("Reached epoch",epoch,"cost J =", cost_)
                
    correct_predictions = tf.equal(tf.argmax(y_,0), tf.argmax(Y,0))
    accuracy = tf.reduce_mean(tf.cast(correct_predictions, "float"))
    accuracy_train = accuracy.eval({X: train, Y: labels_}, session = sess)
    accuracy_test = accuracy.eval({X: test, Y: labels_test_}, session = sess)
                
    return accuracy_train, accuracy_test, sess, cost_history


    



In [85]:

    
sess, cost_history = model (50, 100, train, labels_,logging_step = 10, learning_r = 0.01)


    



In [86]:

    
correct_predictions = tf.equal(tf.argmax(y_,0), tf.argmax(Y,0))
accuracy = tf.reduce_mean(tf.cast(correct_predictions, "float"))
print ("Accuracy:", accuracy.eval({X: train, Y: labels_, learning_rate: 0.001}, session = sess))


    



In [87]:

    
correct_predictions = tf.equal(tf.argmax(y_,0), tf.argmax(Y,0))
accuracy = tf.reduce_mean(tf.cast(correct_predictions, "float"))
print ("Accuracy:", accuracy.eval({X: test, Y: labels_test_, learning_rate: 0.001}, session = sess))


    



In [88]:

    
tf.reset_default_graph() 

n_dim = 784
n1 = 100
n2 = 100
n3 = 100
n4 = 100
n5 = 100
n6 = 100
n7 = 100
n8 = 100
n9 = 100
n10 = 100
n11 = 100
n12 = 100
n_outputs = 9

tf.set_random_seed(5)

X = tf.placeholder(tf.float32, [n_dim, None])
Y = tf.placeholder(tf.float32, [9, None])

learning_rate = tf.placeholder(tf.float32, shape=())

hidden1 = create_layer (X, n1, activation = tf.nn.relu)
hidden2 = create_layer (hidden1, n2, activation = tf.nn.relu)
hidden3 = create_layer (hidden2, n3, activation = tf.nn.relu)
hidden4 = create_layer (hidden3, n4, activation = tf.nn.relu)
hidden5 = create_layer (hidden4, n5, activation = tf.nn.relu)
hidden6 = create_layer (hidden5, n6, activation = tf.nn.relu)
hidden7 = create_layer (hidden6, n7, activation = tf.nn.relu)
hidden8 = create_layer (hidden7, n8, activation = tf.nn.relu)
hidden9 = create_layer (hidden8, n9, activation = tf.nn.relu)
hidden10 = create_layer (hidden9, n10, activation = tf.nn.relu)
hidden11 = create_layer (hidden10, n11, activation = tf.nn.relu)
hidden12 = create_layer (hidden11, n12, activation = tf.nn.relu)
outputs = create_layer (hidden12, n_outputs, activation = tf.identity)
y_ = tf.nn.softmax(outputs)

cost = - tf.reduce_mean(Y * tf.log(y_)+(1-Y) * tf.log(1-y_))

optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)


    



In [89]:

    
def model_layers(minibatch_size, training_epochs, features, classes, logging_step, 
                 learning_r, number_neurons, debug = False):
    
    opt, c, y_, X, Y, learning_rate = build_model_layers(number_neurons)
    
    sess = tf.Session()
    sess.run(tf.global_variables_initializer())

    cost_history = []
    for epoch in range(training_epochs+1):
        for i in range(0, features.shape[1], minibatch_size):
            X_train_mini = features[:,i:i + minibatch_size]
            y_train_mini = classes[:,i:i + minibatch_size]

            #if (i % 5000 == 0):
            #    print('i = ',i)
            
            sess.run(opt, feed_dict = {X: X_train_mini, Y: y_train_mini, learning_rate: learning_r})
        cost_ = sess.run(c, feed_dict={ X:features, Y: classes, learning_rate: learning_r})
        cost_history = np.append(cost_history, cost_)

        if ((epoch % logging_step == 0) & debug):
                print("Reached epoch",epoch,"cost J =", cost_)
                
    correct_predictions = tf.equal(tf.argmax(y_,0), tf.argmax(Y,0))
    accuracy = tf.reduce_mean(tf.cast(correct_predictions, "float"))
    accuracy_train = accuracy.eval({X: train, Y: labels_}, session = sess)
    accuracy_test = accuracy.eval({X: test, Y: labels_test_}, session = sess)
                
    return accuracy_train, accuracy_test, sess, cost_history


    



In [90]:

    
sess, cost_history = model (50, 100, train, labels_,logging_step = 10, learning_r = 0.01)


    



In [91]:

    
correct_predictions = tf.equal(tf.argmax(y_,0), tf.argmax(Y,0))
accuracy = tf.reduce_mean(tf.cast(correct_predictions, "float"))
print ("Accuracy:", accuracy.eval({X: train, Y: labels_, learning_rate: 0.001}, session = sess))


    



In [92]:

    
correct_predictions = tf.equal(tf.argmax(y_,0), tf.argmax(Y,0))
accuracy = tf.reduce_mean(tf.cast(correct_predictions, "float"))
print ("Accuracy:", accuracy.eval({X: test, Y: labels_test_, learning_rate: 0.001}, session = sess))


    



In [93]:

    
tf.reset_default_graph() 

n_dim = 784
n1 = 100
n2 = 100
n3 = 100
n4 = 100
n5 = 100
n6 = 100
n7 = 100
n8 = 100
n9 = 100
n10 = 100
n11 = 100
n12 = 100
n13 = 100
n_outputs = 9

tf.set_random_seed(5)

X = tf.placeholder(tf.float32, [n_dim, None])
Y = tf.placeholder(tf.float32, [9, None])

learning_rate = tf.placeholder(tf.float32, shape=())

hidden1 = create_layer (X, n1, activation = tf.nn.relu)
hidden2 = create_layer (hidden1, n2, activation = tf.nn.relu)
hidden3 = create_layer (hidden2, n3, activation = tf.nn.relu)
hidden4 = create_layer (hidden3, n4, activation = tf.nn.relu)
hidden5 = create_layer (hidden4, n5, activation = tf.nn.relu)
hidden6 = create_layer (hidden5, n6, activation = tf.nn.relu)
hidden7 = create_layer (hidden6, n7, activation = tf.nn.relu)
hidden8 = create_layer (hidden7, n8, activation = tf.nn.relu)
hidden9 = create_layer (hidden8, n9, activation = tf.nn.relu)
hidden10 = create_layer (hidden9, n10, activation = tf.nn.relu)
hidden11 = create_layer (hidden10, n11, activation = tf.nn.relu)
hidden12 = create_layer (hidden11, n12, activation = tf.nn.relu)
hidden13 = create_layer (hidden12, n13, activation = tf.nn.relu)
outputs = create_layer (hidden13, n_outputs, activation = tf.identity)
y_ = tf.nn.softmax(outputs)

cost = - tf.reduce_mean(Y * tf.log(y_)+(1-Y) * tf.log(1-y_))

optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)


    



In [94]:

    
def model_layers(minibatch_size, training_epochs, features, classes, logging_step, 
                 learning_r, number_neurons, debug = False):
    
    opt, c, y_, X, Y, learning_rate = build_model_layers(number_neurons)
    
    sess = tf.Session()
    sess.run(tf.global_variables_initializer())

    cost_history = []
    for epoch in range(training_epochs+1):
        for i in range(0, features.shape[1], minibatch_size):
            X_train_mini = features[:,i:i + minibatch_size]
            y_train_mini = classes[:,i:i + minibatch_size]

            #if (i % 5000 == 0):
            #    print('i = ',i)
            
            sess.run(opt, feed_dict = {X: X_train_mini, Y: y_train_mini, learning_rate: learning_r})
        cost_ = sess.run(c, feed_dict={ X:features, Y: classes, learning_rate: learning_r})
        cost_history = np.append(cost_history, cost_)

        if ((epoch % logging_step == 0) & debug):
                print("Reached epoch",epoch,"cost J =", cost_)
                
    correct_predictions = tf.equal(tf.argmax(y_,0), tf.argmax(Y,0))
    accuracy = tf.reduce_mean(tf.cast(correct_predictions, "float"))
    accuracy_train = accuracy.eval({X: train, Y: labels_}, session = sess)
    accuracy_test = accuracy.eval({X: test, Y: labels_test_}, session = sess)
                
    return accuracy_train, accuracy_test, sess, cost_history


    



In [95]:

    
sess, cost_history = model (50, 100, train, labels_,logging_step = 10, learning_r = 0.01)


    



In [96]:

    
correct_predictions = tf.equal(tf.argmax(y_,0), tf.argmax(Y,0))
accuracy = tf.reduce_mean(tf.cast(correct_predictions, "float"))
print ("Accuracy:", accuracy.eval({X: train, Y: labels_, learning_rate: 0.001}, session = sess))


    



In [97]:

    
correct_predictions = tf.equal(tf.argmax(y_,0), tf.argmax(Y,0))
accuracy = tf.reduce_mean(tf.cast(correct_predictions, "float"))
print ("Accuracy:", accuracy.eval({X: test, Y: labels_test_, learning_rate: 0.001}, session = sess))


    



In [98]:

    
tf.reset_default_graph() 

n_dim = 784
n1 = 100
n2 = 100
n3 = 100
n4 = 100
n5 = 100
n6 = 100
n7 = 100
n8 = 100
n9 = 100
n10 = 100
n11 = 100
n12 = 100
n13 = 100
n14 = 100
n_outputs = 9

tf.set_random_seed(5)

X = tf.placeholder(tf.float32, [n_dim, None])
Y = tf.placeholder(tf.float32, [9, None])

learning_rate = tf.placeholder(tf.float32, shape=())

hidden1 = create_layer (X, n1, activation = tf.nn.relu)
hidden2 = create_layer (hidden1, n2, activation = tf.nn.relu)
hidden3 = create_layer (hidden2, n3, activation = tf.nn.relu)
hidden4 = create_layer (hidden3, n4, activation = tf.nn.relu)
hidden5 = create_layer (hidden4, n5, activation = tf.nn.relu)
hidden6 = create_layer (hidden5, n6, activation = tf.nn.relu)
hidden7 = create_layer (hidden6, n7, activation = tf.nn.relu)
hidden8 = create_layer (hidden7, n8, activation = tf.nn.relu)
hidden9 = create_layer (hidden8, n9, activation = tf.nn.relu)
hidden10 = create_layer (hidden9, n10, activation = tf.nn.relu)
hidden11 = create_layer (hidden10, n11, activation = tf.nn.relu)
hidden12 = create_layer (hidden11, n12, activation = tf.nn.relu)
hidden13 = create_layer (hidden12, n13, activation = tf.nn.relu)
hidden14 = create_layer (hidden13, n14, activation = tf.nn.relu)
outputs = create_layer (hidden14, n_outputs, activation = tf.identity)
y_ = tf.nn.softmax(outputs)

cost = - tf.reduce_mean(Y * tf.log(y_)+(1-Y) * tf.log(1-y_))

optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)


    



In [99]:

    
def model_layers(minibatch_size, training_epochs, features, classes, logging_step, 
                 learning_r, number_neurons, debug = False):
    
    opt, c, y_, X, Y, learning_rate = build_model_layers(number_neurons)
    
    sess = tf.Session()
    sess.run(tf.global_variables_initializer())

    cost_history = []
    for epoch in range(training_epochs+1):
        for i in range(0, features.shape[1], minibatch_size):
            X_train_mini = features[:,i:i + minibatch_size]
            y_train_mini = classes[:,i:i + minibatch_size]

            #if (i % 5000 == 0):
            #    print('i = ',i)
            
            sess.run(opt, feed_dict = {X: X_train_mini, Y: y_train_mini, learning_rate: learning_r})
        cost_ = sess.run(c, feed_dict={ X:features, Y: classes, learning_rate: learning_r})
        cost_history = np.append(cost_history, cost_)

        if ((epoch % logging_step == 0) & debug):
                print("Reached epoch",epoch,"cost J =", cost_)
                
    correct_predictions = tf.equal(tf.argmax(y_,0), tf.argmax(Y,0))
    accuracy = tf.reduce_mean(tf.cast(correct_predictions, "float"))
    accuracy_train = accuracy.eval({X: train, Y: labels_}, session = sess)
    accuracy_test = accuracy.eval({X: test, Y: labels_test_}, session = sess)
                
    return accuracy_train, accuracy_test, sess, cost_history


    



In [100]:

    
sess, cost_history = model (50, 100, train, labels_,logging_step = 10, learning_r = 0.01)


    



In [101]:

    
correct_predictions = tf.equal(tf.argmax(y_,0), tf.argmax(Y,0))
accuracy = tf.reduce_mean(tf.cast(correct_predictions, "float"))
print ("Accuracy:", accuracy.eval({X: train, Y: labels_, learning_rate: 0.001}, session = sess))


    



In [102]:

    
correct_predictions = tf.equal(tf.argmax(y_,0), tf.argmax(Y,0))
accuracy = tf.reduce_mean(tf.cast(correct_predictions, "float"))
print ("Accuracy:", accuracy.eval({X: test, Y: labels_test_, learning_rate: 0.001}, session = sess))


    



In [103]:

    
tf.reset_default_graph() 

n_dim = 784
n1 = 100
n2 = 100
n3 = 100
n4 = 100
n5 = 100
n6 = 100
n7 = 100
n8 = 100
n9 = 100
n10 = 100
n11 = 100
n12 = 100
n13 = 100
n14 = 100
n15 = 100
n_outputs = 9

tf.set_random_seed(5)

X = tf.placeholder(tf.float32, [n_dim, None])
Y = tf.placeholder(tf.float32, [9, None])

learning_rate = tf.placeholder(tf.float32, shape=())

hidden1 = create_layer (X, n1, activation = tf.nn.relu)
hidden2 = create_layer (hidden1, n2, activation = tf.nn.relu)
hidden3 = create_layer (hidden2, n3, activation = tf.nn.relu)
hidden4 = create_layer (hidden3, n4, activation = tf.nn.relu)
hidden5 = create_layer (hidden4, n5, activation = tf.nn.relu)
hidden6 = create_layer (hidden5, n6, activation = tf.nn.relu)
hidden7 = create_layer (hidden6, n7, activation = tf.nn.relu)
hidden8 = create_layer (hidden7, n8, activation = tf.nn.relu)
hidden9 = create_layer (hidden8, n9, activation = tf.nn.relu)
hidden10 = create_layer (hidden9, n10, activation = tf.nn.relu)
hidden11 = create_layer (hidden10, n11, activation = tf.nn.relu)
hidden12 = create_layer (hidden11, n12, activation = tf.nn.relu)
hidden13 = create_layer (hidden12, n13, activation = tf.nn.relu)
hidden14 = create_layer (hidden13, n14, activation = tf.nn.relu)
hidden15 = create_layer (hidden14, n15, activation = tf.nn.relu)
outputs = create_layer (hidden15, n_outputs, activation = tf.identity)
y_ = tf.nn.softmax(outputs)

cost = - tf.reduce_mean(Y * tf.log(y_)+(1-Y) * tf.log(1-y_))

optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)


    



In [104]:

    
def model_layers(minibatch_size, training_epochs, features, classes, logging_step, 
                 learning_r, number_neurons, debug = False):
    
    opt, c, y_, X, Y, learning_rate = build_model_layers(number_neurons)
    
    sess = tf.Session()
    sess.run(tf.global_variables_initializer())

    cost_history = []
    for epoch in range(training_epochs+1):
        for i in range(0, features.shape[1], minibatch_size):
            X_train_mini = features[:,i:i + minibatch_size]
            y_train_mini = classes[:,i:i + minibatch_size]

            #if (i % 5000 == 0):
            #    print('i = ',i)
            
            sess.run(opt, feed_dict = {X: X_train_mini, Y: y_train_mini, learning_rate: learning_r})
        cost_ = sess.run(c, feed_dict={ X:features, Y: classes, learning_rate: learning_r})
        cost_history = np.append(cost_history, cost_)

        if ((epoch % logging_step == 0) & debug):
                print("Reached epoch",epoch,"cost J =", cost_)
                
    correct_predictions = tf.equal(tf.argmax(y_,0), tf.argmax(Y,0))
    accuracy = tf.reduce_mean(tf.cast(correct_predictions, "float"))
    accuracy_train = accuracy.eval({X: train, Y: labels_}, session = sess)
    accuracy_test = accuracy.eval({X: test, Y: labels_test_}, session = sess)
                
    return accuracy_train, accuracy_test, sess, cost_history


    



In [105]:

    
sess, cost_history = model (50, 100, train, labels_,logging_step = 10, learning_r = 0.01)


    



In [106]:

    
correct_predictions = tf.equal(tf.argmax(y_,0), tf.argmax(Y,0))
accuracy = tf.reduce_mean(tf.cast(correct_predictions, "float"))
print ("Accuracy:", accuracy.eval({X: train, Y: labels_, learning_rate: 0.001}, session = sess))


    



In [107]:

    
correct_predictions = tf.equal(tf.argmax(y_,0), tf.argmax(Y,0))
accuracy = tf.reduce_mean(tf.cast(correct_predictions, "float"))
print ("Accuracy:", accuracy.eval({X: test, Y: labels_test_, learning_rate: 0.001}, session = sess))


    



In [108]:

    
tf.reset_default_graph() 

n_dim = 784
n1 = 100
n2 = 100
n3 = 100
n4 = 100
n5 = 100
n6 = 100
n7 = 100
n8 = 100
n9 = 100
n10 = 100
n11 = 100
n12 = 100
n13 = 100
n14 = 100
n15 = 100
n16 = 100
n_outputs = 9

tf.set_random_seed(5)

X = tf.placeholder(tf.float32, [n_dim, None])
Y = tf.placeholder(tf.float32, [9, None])

learning_rate = tf.placeholder(tf.float32, shape=())

hidden1 = create_layer (X, n1, activation = tf.nn.relu)
hidden2 = create_layer (hidden1, n2, activation = tf.nn.relu)
hidden3 = create_layer (hidden2, n3, activation = tf.nn.relu)
hidden4 = create_layer (hidden3, n4, activation = tf.nn.relu)
hidden5 = create_layer (hidden4, n5, activation = tf.nn.relu)
hidden6 = create_layer (hidden5, n6, activation = tf.nn.relu)
hidden7 = create_layer (hidden6, n7, activation = tf.nn.relu)
hidden8 = create_layer (hidden7, n8, activation = tf.nn.relu)
hidden9 = create_layer (hidden8, n9, activation = tf.nn.relu)
hidden10 = create_layer (hidden9, n10, activation = tf.nn.relu)
hidden11 = create_layer (hidden10, n11, activation = tf.nn.relu)
hidden12 = create_layer (hidden11, n12, activation = tf.nn.relu)
hidden13 = create_layer (hidden12, n13, activation = tf.nn.relu)
hidden14 = create_layer (hidden13, n14, activation = tf.nn.relu)
hidden15 = create_layer (hidden14, n15, activation = tf.nn.relu)
hidden16 = create_layer (hidden15, n16, activation = tf.nn.relu)
outputs = create_layer (hidden16, n_outputs, activation = tf.identity)
y_ = tf.nn.softmax(outputs)

cost = - tf.reduce_mean(Y * tf.log(y_)+(1-Y) * tf.log(1-y_))

optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)


    



In [109]:

    
def model_layers(minibatch_size, training_epochs, features, classes, logging_step, 
                 learning_r, number_neurons, debug = False):
    
    opt, c, y_, X, Y, learning_rate = build_model_layers(number_neurons)
    
    sess = tf.Session()
    sess.run(tf.global_variables_initializer())

    cost_history = []
    for epoch in range(training_epochs+1):
        for i in range(0, features.shape[1], minibatch_size):
            X_train_mini = features[:,i:i + minibatch_size]
            y_train_mini = classes[:,i:i + minibatch_size]

            #if (i % 5000 == 0):
            #    print('i = ',i)
            
            sess.run(opt, feed_dict = {X: X_train_mini, Y: y_train_mini, learning_rate: learning_r})
        cost_ = sess.run(c, feed_dict={ X:features, Y: classes, learning_rate: learning_r})
        cost_history = np.append(cost_history, cost_)

        if ((epoch % logging_step == 0) & debug):
                print("Reached epoch",epoch,"cost J =", cost_)
                
    correct_predictions = tf.equal(tf.argmax(y_,0), tf.argmax(Y,0))
    accuracy = tf.reduce_mean(tf.cast(correct_predictions, "float"))
    accuracy_train = accuracy.eval({X: train, Y: labels_}, session = sess)
    accuracy_test = accuracy.eval({X: test, Y: labels_test_}, session = sess)
                
    return accuracy_train, accuracy_test, sess, cost_history


    



In [110]:

    
sess, cost_history = model (50, 100, train, labels_,logging_step = 10, learning_r = 0.01)


    



In [111]:

    
correct_predictions = tf.equal(tf.argmax(y_,0), tf.argmax(Y,0))
accuracy = tf.reduce_mean(tf.cast(correct_predictions, "float"))
print ("Accuracy:", accuracy.eval({X: train, Y: labels_, learning_rate: 0.001}, session = sess))


    



In [112]:

    
correct_predictions = tf.equal(tf.argmax(y_,0), tf.argmax(Y,0))
accuracy = tf.reduce_mean(tf.cast(correct_predictions, "float"))
print ("Accuracy:", accuracy.eval({X: test, Y: labels_test_, learning_rate: 0.001}, session = sess))


    



In [113]:

    
tf.reset_default_graph() 

n_dim = 784
n1 = 100
n2 = 100
n3 = 100
n4 = 100
n5 = 100
n6 = 100
n7 = 100
n8 = 100
n9 = 100
n10 = 100
n11 = 100
n12 = 100
n13 = 100
n14 = 100
n15 = 100
n16 = 100
n17 = 100
n_outputs = 9

tf.set_random_seed(5)

X = tf.placeholder(tf.float32, [n_dim, None])
Y = tf.placeholder(tf.float32, [9, None])

learning_rate = tf.placeholder(tf.float32, shape=())

hidden1 = create_layer (X, n1, activation = tf.nn.relu)
hidden2 = create_layer (hidden1, n2, activation = tf.nn.relu)
hidden3 = create_layer (hidden2, n3, activation = tf.nn.relu)
hidden4 = create_layer (hidden3, n4, activation = tf.nn.relu)
hidden5 = create_layer (hidden4, n5, activation = tf.nn.relu)
hidden6 = create_layer (hidden5, n6, activation = tf.nn.relu)
hidden7 = create_layer (hidden6, n7, activation = tf.nn.relu)
hidden8 = create_layer (hidden7, n8, activation = tf.nn.relu)
hidden9 = create_layer (hidden8, n9, activation = tf.nn.relu)
hidden10 = create_layer (hidden9, n10, activation = tf.nn.relu)
hidden11 = create_layer (hidden10, n11, activation = tf.nn.relu)
hidden12 = create_layer (hidden11, n12, activation = tf.nn.relu)
hidden13 = create_layer (hidden12, n13, activation = tf.nn.relu)
hidden14 = create_layer (hidden13, n14, activation = tf.nn.relu)
hidden15 = create_layer (hidden14, n15, activation = tf.nn.relu)
hidden16 = create_layer (hidden15, n16, activation = tf.nn.relu)
hidden17 = create_layer (hidden16, n17, activation = tf.nn.relu)
outputs = create_layer (hidden17, n_outputs, activation = tf.identity)
y_ = tf.nn.softmax(outputs)

cost = - tf.reduce_mean(Y * tf.log(y_)+(1-Y) * tf.log(1-y_))

optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)


    



In [114]:

    
def model_layers(minibatch_size, training_epochs, features, classes, logging_step, 
                 learning_r, number_neurons, debug = False):
    
    opt, c, y_, X, Y, learning_rate = build_model_layers(number_neurons)
    
    sess = tf.Session()
    sess.run(tf.global_variables_initializer())

    cost_history = []
    for epoch in range(training_epochs+1):
        for i in range(0, features.shape[1], minibatch_size):
            X_train_mini = features[:,i:i + minibatch_size]
            y_train_mini = classes[:,i:i + minibatch_size]

            #if (i % 5000 == 0):
            #    print('i = ',i)
            
            sess.run(opt, feed_dict = {X: X_train_mini, Y: y_train_mini, learning_rate: learning_r})
        cost_ = sess.run(c, feed_dict={ X:features, Y: classes, learning_rate: learning_r})
        cost_history = np.append(cost_history, cost_)

        if ((epoch % logging_step == 0) & debug):
                print("Reached epoch",epoch,"cost J =", cost_)
                
    correct_predictions = tf.equal(tf.argmax(y_,0), tf.argmax(Y,0))
    accuracy = tf.reduce_mean(tf.cast(correct_predictions, "float"))
    accuracy_train = accuracy.eval({X: train, Y: labels_}, session = sess)
    accuracy_test = accuracy.eval({X: test, Y: labels_test_}, session = sess)
                
    return accuracy_train, accuracy_test, sess, cost_history


    



In [115]:

    
sess, cost_history = model (50, 100, train, labels_,logging_step = 10, learning_r = 0.01)


    



In [116]:

    
correct_predictions = tf.equal(tf.argmax(y_,0), tf.argmax(Y,0))
accuracy = tf.reduce_mean(tf.cast(correct_predictions, "float"))
print ("Accuracy:", accuracy.eval({X: train, Y: labels_, learning_rate: 0.001}, session = sess))


    



In [117]:

    
correct_predictions = tf.equal(tf.argmax(y_,0), tf.argmax(Y,0))
accuracy = tf.reduce_mean(tf.cast(correct_predictions, "float"))
print ("Accuracy:", accuracy.eval({X: test, Y: labels_test_, learning_rate: 0.001}, session = sess))


    



In [ ]:

    
 


    

